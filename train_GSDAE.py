#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GSDAEè®­ç»ƒè„šæœ¬
æ ¹æ®æ–¹æ¡ˆæ–‡æ¡£å®ç°çš„æ”¹è¿›ç‰ˆæœ¬è®­ç»ƒç¤ºä¾‹
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# å¯¼å…¥GSDAEæ¨¡å—
from danshen_GSDAE import (
    build_GSDAE, prepare_danshen_data, create_feature_groups,
    analyze_feature_importance, plot_importance_analysis
)

def train_gsdae_model(X_train, X_test, y_train, y_test, feature_groups, 
                      epochs=200, batch_size=32, learning_rate=0.001):
    """
    è®­ç»ƒGSDAEæ¨¡å‹
    """
    print("ğŸš€ å¼€å§‹è®­ç»ƒGSDAEæ¨¡å‹...")
    
    # æ„å»ºæ¨¡å‹
    gsdae_model, feature_selector, encoder_model, predictor_model, group_selective_layer = build_GSDAE(
        input_shape=X_train.shape[1],
        target_dim=y_train.shape[1],
        feature_groups=feature_groups,
        nbr_hidden_layers=3,
        hidden_layer_shape=12,
        encodings_nbr=6,
        group_lasso_rate=0.01,
        l1_rate=0.001,
        dropout_rate=0.2
    )
    
    # ç¼–è¯‘æ¨¡å‹
    gsdae_model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss={
            'reconstruction_select': 'mse',
            'target_prediction': 'mse'
        },
        loss_weights={
            'reconstruction_select': 1.0,
            'target_prediction': 0.5
        },
        metrics=['mae']
    )
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    train_inputs = [X_train, y_train]
    train_outputs = {
        'reconstruction_select': X_train,
        'target_prediction': y_train
    }
    
    test_inputs = [X_test, y_test]
    test_outputs = {
        'reconstruction_select': X_test,
        'target_prediction': y_test
    }
    
    # è®¾ç½®å›è°ƒå‡½æ•°
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=20,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-6,
            verbose=1
        )
    ]
    
    # è®­ç»ƒæ¨¡å‹
    history = gsdae_model.fit(
        train_inputs,
        train_outputs,
        validation_data=(test_inputs, test_outputs),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1
    )
    
    return gsdae_model, feature_selector, encoder_model, predictor_model, group_selective_layer, history

def evaluate_model_performance(gsdae_model, predictor_model, X_test, y_test, scaler_y):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    
    # é¢„æµ‹
    predictions = predictor_model.predict(X_test)
    
    # åæ ‡å‡†åŒ–
    y_test_original = scaler_y.inverse_transform(y_test)
    predictions_original = scaler_y.inverse_transform(predictions)
    
    # è®¡ç®—æŒ‡æ ‡
    mse = np.mean((y_test_original - predictions_original) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(y_test_original - predictions_original))
    
    # è®¡ç®—RÂ²
    ss_res = np.sum((y_test_original - predictions_original) ** 2)
    ss_tot = np.sum((y_test_original - np.mean(y_test_original)) ** 2)
    r2 = 1 - (ss_res / ss_tot)
    
    print(f"ğŸ“ˆ æ¨¡å‹æ€§èƒ½æŒ‡æ ‡:")
    print(f"   MSE: {mse:.4f}")
    print(f"   RMSE: {rmse:.4f}")
    print(f"   MAE: {mae:.4f}")
    print(f"   RÂ²: {r2:.4f}")
    
    return {
        'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2,
        'predictions': predictions_original,
        'actual': y_test_original
    }

def plot_training_history(history):
    """
    ç»˜åˆ¶è®­ç»ƒå†å²
    """
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # æ€»æŸå¤±
    axes[0, 0].plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
    axes[0, 0].plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
    axes[0, 0].set_title('æ€»æŸå¤±')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # é‡æ„æŸå¤±
    if 'reconstruction_select_loss' in history.history:
        axes[0, 1].plot(history.history['reconstruction_select_loss'], label='è®­ç»ƒé‡æ„æŸå¤±')
        axes[0, 1].plot(history.history['val_reconstruction_select_loss'], label='éªŒè¯é‡æ„æŸå¤±')
        axes[0, 1].set_title('é‡æ„æŸå¤±')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
    
    # é¢„æµ‹æŸå¤±
    if 'target_prediction_loss' in history.history:
        axes[1, 0].plot(history.history['target_prediction_loss'], label='è®­ç»ƒé¢„æµ‹æŸå¤±')
        axes[1, 0].plot(history.history['val_target_prediction_loss'], label='éªŒè¯é¢„æµ‹æŸå¤±')
        axes[1, 0].set_title('é¢„æµ‹æŸå¤±')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Loss')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
    
    # å­¦ä¹ ç‡
    if 'lr' in history.history:
        axes[1, 1].plot(history.history['lr'])
        axes[1, 1].set_title('å­¦ä¹ ç‡')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Learning Rate')
        axes[1, 1].set_yscale('log')
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return fig

def main():
    """
    ä¸»è®­ç»ƒæµç¨‹
    """
    print("ğŸ”„ GSDAE (Group Selective Deep AutoEncoder) å®Œæ•´è®­ç»ƒæµç¨‹")
    print("=" * 70)
    
    # æ•°æ®è·¯å¾„
    data_path = '../data/processed_danshen_data.csv'
    
    try:
        # 1. å‡†å¤‡æ•°æ®
        print("ğŸ“Š æ­¥éª¤1: å‡†å¤‡æ•°æ®...")
        feature_data, target_data = prepare_danshen_data(data_path)
        
        print(f"âœ… ç‰¹å¾ç»´åº¦: {feature_data.shape}")
        if target_data is not None:
            print(f"âœ… ç›®æ ‡å˜é‡ç»´åº¦: {target_data.shape}")
            # ä½¿ç”¨æ€»ä¸¹å‚é…®å«é‡ä½œä¸ºä¸»è¦ç›®æ ‡
            if 'SumTS' in target_data.columns:
                main_target = target_data[['SumTS']]
                print("âœ… ä½¿ç”¨SumTSä½œä¸ºç›®æ ‡å˜é‡")
            else:
                main_target = target_data.iloc[:, :1]
                print(f"âœ… ä½¿ç”¨{target_data.columns[0]}ä½œä¸ºç›®æ ‡å˜é‡")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ç›®æ ‡å˜é‡ï¼Œåˆ›å»ºè™šæ‹Ÿç›®æ ‡")
            main_target = pd.DataFrame(np.random.randn(len(feature_data), 1), columns=['dummy_target'])
        
        # 2. åˆ›å»ºç‰¹å¾åˆ†ç»„
        print("\nğŸ“‹ æ­¥éª¤2: åˆ›å»ºç‰¹å¾åˆ†ç»„...")
        feature_groups = create_feature_groups(feature_data.columns.tolist())
        print(f"âœ… åˆ›å»ºäº† {len(feature_groups)} ä¸ªç‰¹å¾ç»„:")
        for group_name, indices in feature_groups.items():
            print(f"   - {group_name}: {len(indices)} ä¸ªç‰¹å¾")
        
        # 3. æ•°æ®é¢„å¤„ç†
        print("\nğŸ”§ æ­¥éª¤3: æ•°æ®é¢„å¤„ç†...")
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        
        X_scaled = scaler_X.fit_transform(feature_data)
        y_scaled = scaler_y.fit_transform(main_target)
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_scaled, test_size=0.2, random_state=42
        )
        
        print(f"âœ… è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
        
        # 4. è®­ç»ƒæ¨¡å‹
        print("\nğŸš€ æ­¥éª¤4: è®­ç»ƒGSDAEæ¨¡å‹...")
        gsdae_model, feature_selector, encoder_model, predictor_model, group_selective_layer, history = train_gsdae_model(
            X_train, X_test, y_train, y_test, feature_groups,
            epochs=100,  # å‡å°‘epochæ•°ä»¥é¿å…é•¿æ—¶é—´è¿è¡Œ
            batch_size=32,
            learning_rate=0.001
        )
        
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
        # 5. è¯„ä¼°æ€§èƒ½
        print("\nğŸ“Š æ­¥éª¤5: è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        performance = evaluate_model_performance(
            gsdae_model, predictor_model, X_test, y_test, scaler_y
        )
        
        # 6. é‡è¦æ€§åˆ†æ
        print("\nğŸ” æ­¥éª¤6: ç‰¹å¾é‡è¦æ€§åˆ†æ...")
        sorted_groups, feature_importance, weights = analyze_feature_importance(
            group_selective_layer, feature_groups, feature_data.columns.tolist()
        )
        
        print("ğŸ“ˆ ç»„é‡è¦æ€§æ’å:")
        for i, (group_name, importance) in enumerate(sorted_groups[:5]):
            print(f"   {i+1}. {group_name}: {importance:.4f}")
        
        # 7. å¯è§†åŒ–ç»“æœ
        print("\nğŸ“Š æ­¥éª¤7: ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        
        # ç»˜åˆ¶è®­ç»ƒå†å²
        plot_training_history(history)
        
        # ç»˜åˆ¶é‡è¦æ€§åˆ†æ
        plot_importance_analysis(sorted_groups, feature_importance)
        
        print("\nğŸ‰ GSDAEè®­ç»ƒå’Œåˆ†æå®Œæˆï¼")
        
        return {
            'model': gsdae_model,
            'performance': performance,
            'importance': {
                'groups': sorted_groups,
                'features': feature_importance,
                'weights': weights
            },
            'history': history
        }
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        print("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ ¼å¼æ˜¯å¦æ­£ç¡®")
        return None

if __name__ == "__main__":
    # æ³¨é‡Šæ‰å®é™…è¿è¡Œä»¥é¿å…é•¿æ—¶é—´æ‰§è¡Œ
    # results = main()
    print("GSDAEè®­ç»ƒè„šæœ¬å·²å‡†å¤‡å®Œæˆï¼")
    print("å–æ¶ˆæ³¨é‡Š main() å‡½æ•°è°ƒç”¨å³å¯å¼€å§‹è®­ç»ƒ")
