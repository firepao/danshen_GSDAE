#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GSDAE (Group Selective Deep AutoEncoder) å®ç°
åŸºäºæ–¹æ¡ˆæ–‡æ¡£çš„æ”¹è¿›ç‰ˆæœ¬ï¼ŒåŒ…å«ï¼š
1. ç»„ç¨€ç–æ­£åˆ™åŒ– (Group Lasso)
2. åŠç›‘ç£å­¦ä¹ æœºåˆ¶
3. é¢„æµ‹å¤´ (Prediction Head)
4. å¤åˆæŸå¤±å‡½æ•°
5. ä¸¤å±‚é‡è¦æ€§åˆ†æ
"""

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import regularizers, optimizers, initializers
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input, Dropout

# è®¾ç½®éšæœºç§å­
tf.random.set_seed(42)
np.random.seed(42)

class ZeroToOneClip(tf.keras.constraints.Constraint):
    """æƒé‡çº¦æŸï¼šé™åˆ¶åœ¨0-1ä¹‹é—´"""
    def __call__(self, w):
        return tf.clip_by_value(w, 0, 1)

class GroupSelectiveLayer(keras.layers.Layer):
    """
    ç»„é€‰æ‹©å±‚ - æ”¯æŒç»„ç¨€ç–æ­£åˆ™åŒ–çš„ç‰¹å¾é€‰æ‹©å±‚
    """
    def __init__(self, feature_groups, group_lasso_rate=0.01, l1_rate=0.001, **kwargs):
        super().__init__(**kwargs)
        self.feature_groups = feature_groups  # ç‰¹å¾åˆ†ç»„ä¿¡æ¯
        self.group_lasso_rate = group_lasso_rate
        self.l1_rate = l1_rate
        
    def build(self, input_shape):
        # ä¸ºæ¯ä¸ªç‰¹å¾åˆ›å»ºæƒé‡
        self.kernel = self.add_weight(
            "kernel", 
            shape=(int(input_shape[-1]),),
            initializer=initializers.RandomUniform(minval=0.999, maxval=1.0),
            constraint=ZeroToOneClip(),
            trainable=True
        )
        
    def call(self, inputs):
        # åº”ç”¨ç‰¹å¾æƒé‡
        weighted_features = tf.multiply(inputs, self.kernel)
        
        # æ·»åŠ ç»„ç¨€ç–æ­£åˆ™åŒ–æŸå¤±
        group_loss = 0.0
        for group_indices in self.feature_groups.values():
            if len(group_indices) > 0:
                group_weights = tf.gather(self.kernel, group_indices)
                group_l2_norm = tf.norm(group_weights, ord=2)
                group_loss += group_l2_norm
        
        # æ·»åŠ L1æ­£åˆ™åŒ–
        l1_loss = tf.reduce_sum(tf.abs(self.kernel))
        
        # å°†æ­£åˆ™åŒ–æŸå¤±æ·»åŠ åˆ°æ¨¡å‹
        self.add_loss(self.group_lasso_rate * group_loss)
        self.add_loss(self.l1_rate * l1_loss)
        
        return weighted_features
    
    def get_config(self):
        config = super().get_config()
        config.update({
            'group_lasso_rate': self.group_lasso_rate,
            'l1_rate': self.l1_rate
        })
        return config

def create_feature_groups(feature_names):
    """
    æ ¹æ®ç‰¹å¾åç§°åˆ›å»ºç‰¹å¾åˆ†ç»„
    æŒ‰ç…§æ–¹æ¡ˆä¸­çš„åˆ†ç»„ï¼šåœŸå£¤å…ƒç´ ã€ä½œç‰©å…ƒç´ ã€åœŸå£¤å…»åˆ†ã€åœ°ç†ä¿¡æ¯ã€æ°”å€™ç¯å¢ƒç­‰
    """
    groups = {
        'åœŸå£¤å…ƒç´ ': [],
        'ä½œç‰©å…ƒç´ ': [],  
        'åœŸå£¤å…»åˆ†': [],
        'åœ°ç†ä¿¡æ¯': [],
        'æ°”å€™ç¯å¢ƒ': [],
        'çœä»½': [],
        'åŸå¸‚': [],
        'åœ°è²Œ': [],
        'åœŸå£¤ç±»å‹': [],
        'æ ½åŸ¹ç±»å‹': [],
        'æ°”å€™ç±»å‹': []
    }
    
    # æ ¹æ®ç‰¹å¾åç§°æ¨¡å¼åŒ¹é…åˆ†ç»„
    for i, name in enumerate(feature_names):
        name_lower = name.lower()
        
        # åœŸå£¤å…ƒç´  (ä»¥_Sç»“å°¾çš„å…ƒç´ )
        if name.endswith('_S'):
            groups['åœŸå£¤å…ƒç´ '].append(i)
        # ä½œç‰©å…ƒç´  (ä»¥_Pç»“å°¾çš„å…ƒç´ )  
        elif name.endswith('_P'):
            groups['ä½œç‰©å…ƒç´ '].append(i)
        # åœŸå£¤å…»åˆ†
        elif any(nutrient in name_lower for nutrient in ['ph', 'om', 'tn', 'tp', 'tk', 'an', 'ap', 'ak']):
            groups['åœŸå£¤å…»åˆ†'].append(i)
        # åœ°ç†ä¿¡æ¯
        elif any(geo in name_lower for geo in ['lat', 'lon', 'alt', 'elevation']):
            groups['åœ°ç†ä¿¡æ¯'].append(i)
        # æ°”å€™ç¯å¢ƒ
        elif any(climate in name_lower for climate in ['temp', 'prec', 'humi', 'wind', 'sun']):
            groups['æ°”å€™ç¯å¢ƒ'].append(i)
        # çœä»½
        elif 'province' in name_lower:
            groups['çœä»½'].append(i)
        # åŸå¸‚
        elif 'city' in name_lower:
            groups['åŸå¸‚'].append(i)
        # åœ°è²Œ
        elif 'landscape' in name_lower:
            groups['åœ°è²Œ'].append(i)
        # åœŸå£¤ç±»å‹
        elif 'soiltype' in name_lower or 'soilclass' in name_lower:
            groups['åœŸå£¤ç±»å‹'].append(i)
        # æ ½åŸ¹ç±»å‹
        elif 'cultivation' in name_lower:
            groups['æ ½åŸ¹ç±»å‹'].append(i)
        # æ°”å€™ç±»å‹
        elif 'climate' in name_lower and 'type' in name_lower:
            groups['æ°”å€™ç±»å‹'].append(i)
    
    # ç§»é™¤ç©ºç»„
    groups = {k: v for k, v in groups.items() if len(v) > 0}
    
    return groups

def build_GSDAE(input_shape, target_dim, feature_groups,
                nbr_hidden_layers=3, hidden_layer_shape=12, 
                encodings_nbr=6, activation="relu",
                group_lasso_rate=0.01, l1_rate=0.001,
                dropout_rate=0.2):
    """
    æ„å»ºGSDAEæ¨¡å‹
    
    å‚æ•°:
    - input_shape: è¾“å…¥ç‰¹å¾ç»´åº¦
    - target_dim: ç›®æ ‡å˜é‡ç»´åº¦ï¼ˆä¸¹å‚é…®å«é‡ï¼‰
    - feature_groups: ç‰¹å¾åˆ†ç»„å­—å…¸
    - å…¶ä»–å‚æ•°: ç½‘ç»œç»“æ„å‚æ•°
    """
    
    # è¾“å…¥å±‚
    feature_inputs = Input(shape=[input_shape], name='feature_input')
    target_inputs = Input(shape=[target_dim], name='target_input')
    
    # ç»„é€‰æ‹©å±‚
    group_selective_layer = GroupSelectiveLayer(
        feature_groups=feature_groups,
        group_lasso_rate=group_lasso_rate,
        l1_rate=l1_rate,
        name='group_selective_layer'
    )
    selected_features = group_selective_layer(feature_inputs)
    
    # ç¼–ç å™¨ - åŸå§‹ç‰¹å¾è·¯å¾„
    encoder_full = feature_inputs
    for i in range(nbr_hidden_layers):
        encoder_full = Dense(
            hidden_layer_shape, 
            activation=activation,
            name=f'encoder_full_{i}'
        )(encoder_full)
        encoder_full = Dropout(dropout_rate)(encoder_full)
    
    # ç¼–ç å™¨ - é€‰æ‹©ç‰¹å¾è·¯å¾„  
    encoder_select = selected_features
    for i in range(nbr_hidden_layers):
        encoder_select = Dense(
            hidden_layer_shape,
            activation=activation, 
            name=f'encoder_select_{i}'
        )(encoder_select)
        encoder_select = Dropout(dropout_rate)(encoder_select)
    
    # ç¼–ç å±‚
    encoding_full = Dense(encodings_nbr, activation=activation, name='encoding_full')(encoder_full)
    encoding_select = Dense(encodings_nbr, activation=activation, name='encoding_select')(encoder_select)
    
    # é¢„æµ‹å¤´ - ç”¨äºåŠç›‘ç£å­¦ä¹ 
    prediction_head = Dense(32, activation='relu', name='pred_hidden')(encoding_select)
    prediction_head = Dropout(dropout_rate)(prediction_head)
    target_prediction = Dense(target_dim, activation='linear', name='target_prediction')(prediction_head)
    
    # è§£ç å™¨
    decoder_full = encoding_full
    decoder_select = encoding_select
    
    for i in range(nbr_hidden_layers):
        decoder_layer = Dense(hidden_layer_shape, activation=activation, name=f'decoder_{i}')
        decoder_full = decoder_layer(decoder_full)
        decoder_select = decoder_layer(decoder_select)
        decoder_full = Dropout(dropout_rate)(decoder_full)
        decoder_select = Dropout(dropout_rate)(decoder_select)
    
    # é‡æ„å±‚
    reconstruction_full = Dense(input_shape, activation='linear', name='reconstruction_full')(decoder_full)
    reconstruction_select = Dense(input_shape, activation='linear', name='reconstruction_select')(decoder_select)
    
    # æ„å»ºä¸åŒçš„æ¨¡å‹
    # å®Œæ•´çš„GSDAEæ¨¡å‹ï¼ˆç”¨äºè®­ç»ƒï¼‰
    gsdae_model = Model(
        inputs=[feature_inputs, target_inputs],
        outputs=[reconstruction_select, target_prediction],
        name='GSDAE'
    )
    
    # ç‰¹å¾é€‰æ‹©æ¨¡å‹
    feature_selector = Model(
        inputs=feature_inputs,
        outputs=selected_features,
        name='FeatureSelector'
    )
    
    # ç¼–ç å™¨æ¨¡å‹
    encoder_model = Model(
        inputs=feature_inputs,
        outputs=encoding_select,
        name='Encoder'
    )
    
    # é¢„æµ‹æ¨¡å‹
    predictor_model = Model(
        inputs=feature_inputs,
        outputs=target_prediction,
        name='Predictor'
    )
    
    return gsdae_model, feature_selector, encoder_model, predictor_model, group_selective_layer

def custom_loss_function(reconstruction_weight=1.0, prediction_weight=0.5):
    """
    è‡ªå®šä¹‰å¤åˆæŸå¤±å‡½æ•°
    åŒ…å«é‡å»ºè¯¯å·®å’Œé¢„æµ‹è¯¯å·®
    """
    def loss(y_true, y_pred):
        # y_trueå’Œy_predéƒ½æ˜¯åˆ—è¡¨ï¼ŒåŒ…å«[reconstruction_target, prediction_target]
        reconstruction_target, prediction_target = y_true
        reconstruction_pred, prediction_pred = y_pred
        
        # é‡å»ºæŸå¤±
        reconstruction_loss = tf.reduce_mean(tf.square(reconstruction_target - reconstruction_pred))
        
        # é¢„æµ‹æŸå¤±  
        prediction_loss = tf.reduce_mean(tf.square(prediction_target - prediction_pred))
        
        # å¤åˆæŸå¤±
        total_loss = reconstruction_weight * reconstruction_loss + prediction_weight * prediction_loss
        
        return total_loss
    
    return loss

def analyze_feature_importance(group_selective_layer, feature_groups, feature_names):
    """
    ä¸¤å±‚é‡è¦æ€§åˆ†æ
    1. ç»„ï¼ˆç±»åˆ«ï¼‰é‡è¦æ€§è¯„ä¼°
    2. ç»„å†…å…³é”®ç‰¹å¾è¯†åˆ«
    """
    # è·å–é€‰æ‹©å±‚æƒé‡
    weights = group_selective_layer.kernel.numpy()

    # ç¬¬ä¸€å±‚ï¼šç»„é‡è¦æ€§è¯„ä¼°
    group_importance = {}
    for group_name, indices in feature_groups.items():
        if len(indices) > 0:
            group_weights = weights[indices]
            # è®¡ç®—L2èŒƒæ•°ä½œä¸ºç»„é‡è¦æ€§
            group_l2_norm = np.linalg.norm(group_weights, ord=2)
            group_importance[group_name] = group_l2_norm

    # æŒ‰é‡è¦æ€§æ’åº
    sorted_groups = sorted(group_importance.items(), key=lambda x: x[1], reverse=True)

    # ç¬¬äºŒå±‚ï¼šç»„å†…å…³é”®ç‰¹å¾è¯†åˆ«
    feature_importance = {}
    for group_name, indices in feature_groups.items():
        if len(indices) > 0:
            group_weights = weights[indices]
            group_features = [feature_names[i] for i in indices]

            # æŒ‰æƒé‡æ’åºç»„å†…ç‰¹å¾
            feature_weight_pairs = list(zip(group_features, group_weights))
            sorted_features = sorted(feature_weight_pairs, key=lambda x: x[1], reverse=True)

            feature_importance[group_name] = sorted_features

    return sorted_groups, feature_importance, weights

def plot_importance_analysis(sorted_groups, feature_importance, top_n_groups=5, top_n_features=3):
    """
    å¯è§†åŒ–é‡è¦æ€§åˆ†æç»“æœ
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # ç»˜åˆ¶ç»„é‡è¦æ€§
    groups = [item[0] for item in sorted_groups[:top_n_groups]]
    importance = [item[1] for item in sorted_groups[:top_n_groups]]

    ax1.barh(groups, importance)
    ax1.set_xlabel('ç»„é‡è¦æ€§ (L2èŒƒæ•°)')
    ax1.set_title('ç‰¹å¾ç»„é‡è¦æ€§æ’å')
    ax1.grid(True, alpha=0.3)

    # ç»˜åˆ¶å…³é”®ç‰¹å¾ï¼ˆæ¥è‡ªæœ€é‡è¦çš„ç»„ï¼‰
    if sorted_groups:
        top_group = sorted_groups[0][0]
        top_features = feature_importance[top_group][:top_n_features]

        feature_names = [item[0] for item in top_features]
        feature_weights = [item[1] for item in top_features]

        ax2.barh(feature_names, feature_weights)
        ax2.set_xlabel('ç‰¹å¾æƒé‡')
        ax2.set_title(f'"{top_group}"ç»„å†…å…³é”®ç‰¹å¾')
        ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return fig

def prepare_danshen_data(data_path):
    """
    å‡†å¤‡ä¸¹å‚æ•°æ®ï¼ŒåŒ…æ‹¬ç‰¹å¾å’Œç›®æ ‡å˜é‡çš„åˆ†ç¦»
    """
    # è¯»å–æ•°æ®
    data = pd.read_csv(data_path)

    # åˆ é™¤å‰ä¸‰åˆ—
    data = data.iloc[:, 3:]

    # å®šä¹‰ç›®æ ‡å˜é‡ï¼ˆä¸¹å‚é…®ç›¸å…³æˆåˆ†ï¼‰
    target_columns = [
        'CS', 'MT', 'TSIIA', 'TSI', 'DTSI', 'SumTS', 'PD', 'CFA', 'FA', 'SAD', 'SF', 'DSS',
        'SAC', 'SAE', 'MCF', 'RA', 'SAA', 'LA', 'SAY', 'TA', 'CTA', 'MA', 'FMA', 'SUA', 'SAB'
    ]

    # åˆ†ç¦»ç›®æ ‡å˜é‡å’Œç‰¹å¾
    available_targets = [col for col in target_columns if col in data.columns]
    target_data = data[available_targets] if available_targets else None

    # åˆ é™¤ç›®æ ‡å˜é‡å’Œå…¶ä»–ä¸éœ€è¦çš„åˆ—
    drop_cols = available_targets + ['Soil_sampleN', 'etestN', 'testNp', 'etestpatch']
    feature_data = data.drop(columns=[col for col in drop_cols if col in data.columns])

    # åˆ é™¤ç©ºç™½å€¼è¾ƒå¤šçš„åˆ—
    thresh = len(feature_data) * 0.5
    feature_data = feature_data.dropna(axis=1, thresh=thresh)

    # å¦‚æœæœ‰ç›®æ ‡å˜é‡ï¼Œä¹Ÿè¦å¯¹åº”åˆ é™¤ç¼ºå¤±æ ·æœ¬
    if target_data is not None:
        # æ‰¾åˆ°ç‰¹å¾æ•°æ®çš„æœ‰æ•ˆç´¢å¼•
        valid_indices = feature_data.dropna().index
        feature_data = feature_data.loc[valid_indices]
        target_data = target_data.loc[valid_indices]

        # åˆ é™¤ç›®æ ‡å˜é‡ä¸­çš„ç¼ºå¤±å€¼
        target_valid_indices = target_data.dropna().index
        feature_data = feature_data.loc[target_valid_indices]
        target_data = target_data.loc[target_valid_indices]
    else:
        feature_data = feature_data.dropna()

    # ç‹¬çƒ­ç¼–ç åˆ†ç±»ç‰¹å¾
    categorical_columns = [
        "Province", "City", "Microb", "Landscape", "SoilType", "soilclass",
        "CultivationType", "ClimateType", "æŒ‰æ°”å€™èšç±»åˆ’åˆ†çš„ç±»å‹"
    ]
    categorical_columns = [col for col in categorical_columns if col in feature_data.columns]
    feature_data = pd.get_dummies(feature_data, columns=categorical_columns, drop_first=True)

    return feature_data, target_data

def main_training_example():
    """
    ä¸»è®­ç»ƒç¤ºä¾‹ - å±•ç¤ºå¦‚ä½•ä½¿ç”¨GSDAE
    """
    print("ğŸ”„ GSDAE (Group Selective Deep AutoEncoder) è®­ç»ƒç¤ºä¾‹")
    print("=" * 60)

    # æ•°æ®è·¯å¾„
    data_path = 'D:/è¯¾é¢˜ä¼š/ä¸¹å‚/danshen_code/SDAE/SDAE-main/data/ä¸¹å‚æ•°æ®salvia_all_20240425 - å‰¯æœ¬.csv'

    # å‡†å¤‡æ•°æ®
    print("ğŸ“Š å‡†å¤‡æ•°æ®...")
    feature_data, target_data = prepare_danshen_data(data_path)

    print(f"ç‰¹å¾ç»´åº¦: {feature_data.shape}")
    if target_data is not None:
        print(f"ç›®æ ‡å˜é‡ç»´åº¦: {target_data.shape}")
        # ä½¿ç”¨æ€»ä¸¹å‚é…®å«é‡ä½œä¸ºä¸»è¦ç›®æ ‡ï¼ˆå¦‚æœæœ‰SumTSåˆ—ï¼‰
        if 'SumTS' in target_data.columns:
            main_target = target_data[['SumTS']]
        else:
            main_target = target_data.iloc[:, :1]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç›®æ ‡å˜é‡
    else:
        print("âš ï¸ æœªæ‰¾åˆ°ç›®æ ‡å˜é‡ï¼Œå°†ä½¿ç”¨æ— ç›‘ç£æ¨¡å¼")
        main_target = np.zeros((len(feature_data), 1))

    # åˆ›å»ºç‰¹å¾åˆ†ç»„
    feature_groups = create_feature_groups(feature_data.columns.tolist())
    print(f"ğŸ“‹ ç‰¹å¾åˆ†ç»„: {list(feature_groups.keys())}")

    # æ•°æ®æ ‡å‡†åŒ–
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()

    X_scaled = scaler_X.fit_transform(feature_data)
    y_scaled = scaler_y.fit_transform(main_target)

    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_scaled, test_size=0.2, random_state=42
    )

    print(f"è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    # æ„å»ºæ¨¡å‹
    print("ğŸ—ï¸ æ„å»ºGSDAEæ¨¡å‹...")
    gsdae_model, feature_selector, encoder_model, predictor_model, group_selective_layer = build_GSDAE(
        input_shape=X_train.shape[1],
        target_dim=y_train.shape[1],
        feature_groups=feature_groups,
        nbr_hidden_layers=3,
        hidden_layer_shape=12,
        encodings_nbr=6,
        group_lasso_rate=0.01,
        l1_rate=0.001
    )

    print("âœ… æ¨¡å‹æ„å»ºå®Œæˆï¼")
    print(f"ğŸ“ˆ å¯è¿›è¡Œè®­ç»ƒå’Œé‡è¦æ€§åˆ†æ")

    return {
        'model': gsdae_model,
        'feature_selector': feature_selector,
        'encoder_model': encoder_model,
        'predictor_model': predictor_model,
        'group_selective_layer': group_selective_layer,
        'feature_groups': feature_groups,
        'feature_names': feature_data.columns.tolist(),
        'data': {
            'X_train': X_train, 'X_test': X_test,
            'y_train': y_train, 'y_test': y_test
        },
        'scalers': {'X': scaler_X, 'y': scaler_y}
    }

if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹ï¼ˆæ³¨é‡Šæ‰ä»¥é¿å…å®é™…æ‰§è¡Œï¼‰
    # results = main_training_example()
    print("GSDAEæ¨¡å‹ä»£ç å·²å‡†å¤‡å®Œæˆï¼")
    print("ä¸»è¦æ”¹è¿›:")
    print("1. âœ… ç»„ç¨€ç–æ­£åˆ™åŒ– (Group Lasso)")
    print("2. âœ… åŠç›‘ç£å­¦ä¹ æœºåˆ¶")
    print("3. âœ… é¢„æµ‹å¤´ (Prediction Head)")
    print("4. âœ… å¤åˆæŸå¤±å‡½æ•°")
    print("5. âœ… ä¸¤å±‚é‡è¦æ€§åˆ†æ")
    print("6. âœ… ç‰¹å¾åˆ†ç»„ç»“æ„")
