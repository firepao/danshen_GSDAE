{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSDAE (Group Selective Deep AutoEncoder) for Danshen Analysis\n",
    "\n",
    "基于方案文档的改进版本，包含：\n",
    "1. 组稀疏正则化 (Group Lasso)\n",
    "2. 半监督学习机制\n",
    "3. 预测头 (Prediction Head)\n",
    "4. 复合损失函数\n",
    "5. 两层重要性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras import optimizers, initializers\n",
    "from tensorflow.keras.callbacks import TerminateOnNaN, ReduceLROnPlateau\n",
    "\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据形状: (991, 159)\n",
      "删除前三列后数据形状: (991, 156)\n",
      "特征数据形状: (991, 127)\n",
      "目标数据形状: (991, 25)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('../data/丹参数据salvia_all_20240425 - 副本.csv')\n",
    "print(f\"原始数据形状: {data.shape}\")\n",
    "\n",
    "# 删除前三列\n",
    "data = data.iloc[:, 3:]\n",
    "print(f\"删除前三列后数据形状: {data.shape}\")\n",
    "\n",
    "# 定义目标变量（丹参酮相关成分）\n",
    "target_columns = [\n",
    "    'CS', 'MT', 'TSIIA', 'TSI', 'DTSI', 'SumTS', 'PD', 'CFA', 'FA', 'SAD', 'SF', 'DSS',\n",
    "    'SAC', 'SAE', 'MCF', 'RA', 'SAA', 'LA', 'SAY', 'TA', 'CTA', 'MA', 'FMA', 'SUA', 'SAB'\n",
    "]\n",
    "\n",
    "# 分离目标变量和特征\n",
    "available_targets = [col for col in target_columns if col in data.columns]\n",
    "target_data = data[available_targets] if available_targets else None\n",
    "\n",
    "# 删除目标变量和其他不需要的列\n",
    "drop_cols = available_targets + ['Soil_sampleN', 'etestN', 'testNp', 'etestpatch']\n",
    "feature_data = data.drop(columns=[col for col in drop_cols if col in data.columns])\n",
    "\n",
    "print(f\"特征数据形状: {feature_data.shape}\")\n",
    "print(f\"目标数据形状: {target_data.shape if target_data is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用SumTS作为主要目标变量\n",
      "最终特征数据形状: (496, 141)\n",
      "目标变量形状: (496, 1)\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "thresh = len(feature_data) * 0.5\n",
    "feature_data = feature_data.dropna(axis=1, thresh=thresh)\n",
    "\n",
    "if target_data is not None:\n",
    "    valid_indices = feature_data.dropna().index\n",
    "    feature_data = feature_data.loc[valid_indices]\n",
    "    target_data = target_data.loc[valid_indices]\n",
    "    \n",
    "    target_valid_indices = target_data.dropna().index\n",
    "    feature_data = feature_data.loc[target_valid_indices]\n",
    "    target_data = target_data.loc[target_valid_indices]\n",
    "else:\n",
    "    feature_data = feature_data.dropna()\n",
    "\n",
    "# 独热编码\n",
    "categorical_columns = [\"Province\", \"City\", \"Microb\", \"Landscape\", \"SoilType\", \"soilclass\", \"CultivationType\", \"ClimateType\", \"按气候聚类划分的类型\"]\n",
    "categorical_columns = [col for col in categorical_columns if col in feature_data.columns]\n",
    "feature_data = pd.get_dummies(feature_data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# 准备目标变量\n",
    "if target_data is not None and 'SumTS' in target_data.columns:\n",
    "    main_target = target_data[['SumTS']]\n",
    "    print(\"使用SumTS作为主要目标变量\")\n",
    "elif target_data is not None:\n",
    "    main_target = target_data.iloc[:, :1]\n",
    "    print(f\"使用{target_data.columns[0]}作为主要目标变量\")\n",
    "else:\n",
    "    main_target = pd.DataFrame(np.random.randn(len(feature_data), 1), columns=['dummy_target'])\n",
    "    print(\"创建虚拟目标变量\")\n",
    "\n",
    "print(f\"最终特征数据形状: {feature_data.shape}\")\n",
    "print(f\"目标变量形状: {main_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: (396, 141), 测试集: (100, 141)\n",
      "特征总数: 141\n"
     ]
    }
   ],
   "source": [
    "# 数据分割和标准化\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_data, main_target, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scal = scaler_X.fit_transform(X_train)\n",
    "X_test_scal = scaler_X.transform(X_test)\n",
    "y_train_scal = scaler_y.fit_transform(y_train)\n",
    "y_test_scal = scaler_y.transform(y_test)\n",
    "\n",
    "feature_names = list(feature_data.columns)\n",
    "print(f\"训练集: {X_train_scal.shape}, 测试集: {X_test_scal.shape}\")\n",
    "print(f\"特征总数: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建了 5 个特征组:\n",
      "  - 必需元素: 8 个特征\n",
      "  - 有益元素: 3 个特征\n",
      "  - 重金属元素: 6 个特征\n",
      "  - 稀土元素: 14 个特征\n",
      "  - 其他元素: 14 个特征\n"
     ]
    }
   ],
   "source": [
    "def create_feature_groups(feature_names):\n",
    "    \"\"\"根据特征名称创建特征分组\"\"\"\n",
    "    \n",
    "    \n",
    "    # 定义元素分组\n",
    "    ee_elements = {'mn', 'cu', 'fe', 'zn', 'mg', 'k', 'ca', 'mo'} # 必需元素\n",
    "    be_elements = {'v', 'na', 'co'} # 有益元素\n",
    "    hm_elements = {'pb', 'cr', 'cd', 'as', 'sb', 'sn'} # 重金属元素\n",
    "    re_elements = {'y', 'la', 'ce', 'pr', 'nd', 'sm', 'eu', 'gd', 'tb', 'dy', 'ho', 'er', 'yb', 'th'} # 稀土元素\n",
    "    oe_elements = {'ga', 'rb', 'li', 'sr', 'ba', 'be', 'cs', 'ni', 'sc', 'ge', 'hf', 'nb', 'ti', 'zr'} # 其他元素\n",
    "    \n",
    "    \n",
    "    groups = {\n",
    "        '必需元素': [],\n",
    "        '有益元素': [],\n",
    "        '重金属元素': [],\n",
    "        '稀土元素': [],\n",
    "        '其他元素': [],\n",
    "        '土壤元素': [],  \n",
    "        '理化性质': [],\n",
    "        '地理信息': [],\n",
    "        '气候环境': [],\n",
    "        '理化+元素':[]\n",
    "    }\n",
    "    \n",
    "    for i, name in enumerate(feature_names):\n",
    "        name_lower = name.lower()\n",
    "        \n",
    "        # if any(climate in name_lower for climate in ['di','gdd0','gdd5','gp','mi','mtco','mtwa','tmax','tmin']):\n",
    "        #     groups['气候环境'].append(i)\n",
    "        # elif name.endswith('_S'):\n",
    "        #     groups['土壤元素'].append(i)\n",
    "        # elif any(nutrient in name_lower for nutrient in ['ph', 'ec', 'nh4_n', 'no3_n', 'n.n', 'tn', 'om', 'soiltypenum', 'an_x', 'ap_x', 'ak_x']):\n",
    "        #     groups['理化性质'].append(i)\n",
    "        # elif any(geo in name_lower for geo in ['alt_extract', 'longitude', 'latitude']):\n",
    "        #     groups['地理信息'].append(i)\n",
    "        \n",
    "        # if any(climate in name_lower for climate in ['di','gdd0','gdd5','gp','mi','mtco','mtwa','tmax','tmin']):\n",
    "        #     groups['气候环境'].append(i)\n",
    "        # if name.endswith('_S'):\n",
    "        #     groups['理化+元素'].append(i)\n",
    "        # elif any(nutrient in name_lower for nutrient in ['ph', 'ec', 'nh4_n', 'no3_n', 'n.n', 'tn', 'om', 'soiltypenum', 'an_x', 'ap_x', 'ak_x']):\n",
    "        #     groups['理化+元素'].append(i)\n",
    "        \n",
    "        # 检查是否为土壤元素\n",
    "        if name_lower.endswith('_s'):\n",
    "            element = name_lower.split('_')[0]\n",
    "            if element in ee_elements:\n",
    "                groups['必需元素'].append(i)\n",
    "            elif element in be_elements:\n",
    "                groups['有益元素'].append(i)\n",
    "            elif element in hm_elements:\n",
    "                groups['重金属元素'].append(i)\n",
    "            elif element in re_elements:\n",
    "                groups['稀土元素'].append(i)\n",
    "            elif element in oe_elements:\n",
    "                groups['其他元素'].append(i)\n",
    "            # 未分类的元素可以放在“其他”或单独处理，这里忽略\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    # 移除空组\n",
    "    groups = {k: v for k, v in groups.items() if len(v) > 0}\n",
    "    return groups\n",
    "\n",
    "# 创建特征分组\n",
    "feature_groups = create_feature_groups(feature_names)\n",
    "print(f\"创建了 {len(feature_groups)} 个特征组:\")\n",
    "for group_name, indices in feature_groups.items():\n",
    "    print(f\"  - {group_name}: {len(indices)} 个特征\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSDAE Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改初始化范围\n",
    "添加数值稳定性检查\n",
    "        group_loss = tf.clip_by_value(group_loss, 0.0, 1e6)\n",
    "        l1_loss = tf.clip_by_value(l1_loss, 0.0, 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroToOneClip(tf.keras.constraints.Constraint):\n",
    "    \"\"\"权重约束：限制在0-1之间\"\"\"\n",
    "    def __call__(self, w):\n",
    "        return tf.clip_by_value(w, 0, 1)\n",
    "\n",
    "class GroupSelectiveLayer(keras.layers.Layer):\n",
    "    \"\"\"组选择层 - 支持组稀疏正则化的特征选择层\"\"\"\n",
    "    def __init__(self, feature_groups, group_lasso_rate=0.01, l1_rate=0.001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_groups = feature_groups\n",
    "        self.group_lasso_rate = group_lasso_rate\n",
    "        self.l1_rate = l1_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\", \n",
    "            shape=(int(input_shape[-1]),),\n",
    "            \n",
    "            # 修改初始化范围\n",
    "            initializer=initializers.RandomUniform(minval=0.5, maxval=1.0),  # 修改初始化范围\n",
    "            constraint=ZeroToOneClip(),\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        weighted_features = tf.multiply(inputs, self.kernel)\n",
    "        \n",
    "        # 添加组稀疏正则化损失\n",
    "        group_loss = 0.0\n",
    "        for group_indices in self.feature_groups.values():\n",
    "            if len(group_indices) > 0:\n",
    "                group_weights = tf.gather(self.kernel, group_indices)\n",
    "                group_l2_norm = tf.norm(group_weights, ord=2)\n",
    "                group_loss += group_l2_norm\n",
    "        \n",
    "        l1_loss = tf.reduce_sum(tf.abs(self.kernel))\n",
    "        \n",
    "        # 添加数值稳定性检查\n",
    "        group_loss = tf.clip_by_value(group_loss, 0.0, 1e6)\n",
    "        l1_loss = tf.clip_by_value(l1_loss, 0.0, 1e6)\n",
    "        \n",
    "        self.add_loss(self.group_lasso_rate * group_loss)\n",
    "        self.add_loss(self.l1_rate * l1_loss)\n",
    "        \n",
    "        return weighted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_GSDAE(input_shape, target_dim, feature_groups, nbr_hidden_layers=3, hidden_layer_shape=12, encodings_nbr=6, activation=\"relu\", group_lasso_rate=0.01, l1_rate=0.001):\n",
    "    \"\"\"构建GSDAE模型\"\"\"\n",
    "    \n",
    "    # 输入层\n",
    "    feature_inputs = Input(shape=[input_shape], name='input')\n",
    "    \n",
    "    # 组选择层\n",
    "    group_selective_layer = GroupSelectiveLayer(\n",
    "        feature_groups=feature_groups,\n",
    "        group_lasso_rate=group_lasso_rate,\n",
    "        l1_rate=l1_rate,\n",
    "        name='group_selective_layer'\n",
    "    )\n",
    "    selected_features = group_selective_layer(feature_inputs)\n",
    "    \n",
    "    # 编码器 - 原始特征路径\n",
    "    encoder_full = feature_inputs\n",
    "    for i in range(nbr_hidden_layers):\n",
    "        encoder_full = Dense(hidden_layer_shape, activation=activation, name=f'encoder_hidden_layer_full_{i}')(encoder_full)\n",
    "    \n",
    "    # 编码器 - 选择特征路径  \n",
    "    encoder_select = selected_features\n",
    "    for i in range(nbr_hidden_layers):\n",
    "        encoder_select = Dense(hidden_layer_shape, activation=activation, name=f'encoder_hidden_layer_select_{i}')(encoder_select)\n",
    "    \n",
    "    # 编码层\n",
    "    encoding_full = Dense(encodings_nbr, activation=activation, name='encoding_layer_full')(encoder_full)\n",
    "    encoding_select = Dense(encodings_nbr, activation=activation, name='encoding_layer_select')(encoder_select)\n",
    "    \n",
    "    # 预测头 - 用于半监督学习\n",
    "    prediction_head = Dense(32, activation='relu', name='pred_hidden')(encoding_select)\n",
    "    target_prediction = Dense(target_dim, activation='linear', name='target_prediction')(prediction_head)\n",
    "    \n",
    "    # 解码器 - 共享权重\n",
    "    decoder_layers = []\n",
    "    for i in range(nbr_hidden_layers):\n",
    "        decoder_layer = Dense(hidden_layer_shape, activation=activation, name=f'decoder_hidden_layer_{i}')\n",
    "        decoder_layers.append(decoder_layer)\n",
    "    \n",
    "    reconstruction_layer = Dense(input_shape, activation='linear', name='reconstruction_layer')\n",
    "    \n",
    "    # 应用解码器\n",
    "    decoder_full = encoding_full\n",
    "    decoder_select = encoding_select\n",
    "    \n",
    "    for decoder_layer in decoder_layers:\n",
    "        decoder_full = decoder_layer(decoder_full)\n",
    "        decoder_select = decoder_layer(decoder_select)\n",
    "    \n",
    "    # 重构输出\n",
    "    reconstruction_full = reconstruction_layer(decoder_full)\n",
    "    reconstruction_select = reconstruction_layer(decoder_select)\n",
    "    \n",
    "    # 构建模型\n",
    "    gsdae_model = Model(inputs=feature_inputs, outputs=[reconstruction_select, target_prediction], name='GSDAE')\n",
    "    full_autoencoder = Model(inputs=feature_inputs, outputs=reconstruction_full, name='FullAutoEncoder')\n",
    "    feature_selector = Model(inputs=feature_inputs, outputs=selected_features, name='FeatureSelector')\n",
    "    full_encoder = Model(inputs=feature_inputs, outputs=encoding_full, name='FullEncoder')\n",
    "    select_encoder = Model(inputs=feature_inputs, outputs=encoding_select, name='SelectEncoder')\n",
    "    \n",
    "    return gsdae_model, full_autoencoder, feature_selector, full_encoder, select_encoder, group_selective_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lasso_rate = 1e-4  # 降低L1正则化率\n",
    "group_lasso_rate = 1e-3  # 降低组稀疏正则化率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_for_tuning(hp):\n",
    "    \"\"\"用于KerasTuner的模型构建函数\"\"\"\n",
    "    # 1. 定义超参数的搜索范围\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-4, 1e-3, 5e-3])\n",
    "    hp_group_lasso_rate = hp.Float('group_lasso_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    hp_l1_rate = hp.Float('l1_rate', min_value=1e-6, max_value=1e-3, sampling='log')\n",
    "    hp_prediction_weight = hp.Choice('prediction_weight', values=[0.1, 0.2, 0.5, 1.0])\n",
    "\n",
    "    # 2. 使用这些超参数构建模型\n",
    "    # 注意：这里我们复用你已经写好的 build_GSDAE 函数\n",
    "    gsdae_model, _, _, _, _, _ = build_GSDAE(\n",
    "        input_shape=X_train_scal.shape[1],\n",
    "        target_dim=y_train_scal.shape[1],\n",
    "        feature_groups=feature_groups,\n",
    "        nbr_hidden_layers=3,  # 固定结构参数\n",
    "        hidden_layer_shape=12, # 固定结构参数\n",
    "        encodings_nbr=6,       # 固定结构参数\n",
    "        activation=\"relu\",\n",
    "        group_lasso_rate=hp_group_lasso_rate,\n",
    "        l1_rate=hp_l1_rate\n",
    "    )\n",
    "\n",
    "    # 3. 编译模型\n",
    "    optimizer = optimizers.Adam(learning_rate=hp_learning_rate, clipnorm=1.0)\n",
    "    gsdae_model.compile(\n",
    "        loss={'reconstruction_layer': 'mean_squared_error', 'target_prediction': 'mean_squared_error'},\n",
    "        loss_weights={'reconstruction_layer': 1.0, 'target_prediction': hp_prediction_weight},\n",
    "        optimizer=optimizer,\n",
    "        metrics={'target_prediction': 'mae'} # 监控预测任务的mae\n",
    "    )\n",
    "    \n",
    "    return gsdae_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说是要注释掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 参数设置\n",
    "# lasso_rate = 1e-4  # 降低L1正则化率\n",
    "# group_lasso_rate = 1e-3  # 降低组稀疏正则化率\n",
    "# hidden_layer_shape = 12\n",
    "# nbr_hidden_layers = 3\n",
    "# encodings_nbr = 6\n",
    "# activation = \"relu\"\n",
    "# learning_rate = 1e-3\n",
    "# nbr_batches = 20\n",
    "# batch_size = int(np.floor(X_train_scal.shape[0]/nbr_batches))\n",
    "# seed = 0\n",
    "\n",
    "# # 设置随机种子\n",
    "# random.seed(seed)\n",
    "# rndm_seed = random.randint(1,10000)\n",
    "# tf.random.set_seed(rndm_seed)\n",
    "\n",
    "# print(f\"批次大小: {batch_size}\")\n",
    "# print(f\"随机种子: {rndm_seed}\")\n",
    "# print(f\"L1正则化率: {lasso_rate}\")\n",
    "# print(f\"组稀疏正则化率: {group_lasso_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 构建GSDAE模型\n",
    "# GSDAE, FullAutoEncoder, FeatureSelector, FullEncoder, SelectEncoder, GroupSelectiveLayer = build_GSDAE(\n",
    "#     input_shape=X_train_scal.shape[1],\n",
    "#     target_dim=y_train_scal.shape[1],\n",
    "#     feature_groups=feature_groups,\n",
    "#     nbr_hidden_layers=nbr_hidden_layers,\n",
    "#     hidden_layer_shape=hidden_layer_shape,\n",
    "#     encodings_nbr=encodings_nbr,\n",
    "#     activation=activation,\n",
    "#     group_lasso_rate=group_lasso_rate,\n",
    "#     l1_rate=lasso_rate\n",
    "# )\n",
    "\n",
    "# print(\"FullAutoEncoder Structure-------------------------------------\")\n",
    "# FullAutoEncoder.summary()\n",
    "# print(\"GSDAE Structure-------------------------------------\")\n",
    "# GSDAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training: Full AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 预训练全特征自编码器\n",
    "# FullAutoEncoder.compile(loss='mean_squared_error', optimizer=optimizers.Adam(learning_rate=learning_rate))\n",
    "# history_pretrain = FullAutoEncoder.fit(\n",
    "#     X_train_scal, X_train_scal, \n",
    "#     epochs=400, \n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     validation_data=(X_test_scal, X_test_scal),\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # 评估预训练模型\n",
    "# train_pred_full = FullAutoEncoder.predict(X_train_scal)\n",
    "# test_pred_full = FullAutoEncoder.predict(X_test_scal)\n",
    "# train_mse_full = mean_squared_error(X_train_scal, train_pred_full)\n",
    "# test_mse_full = mean_squared_error(X_test_scal, test_pred_full)\n",
    "# print(f\"Full AutoEncoder - 训练MSE: {train_mse_full:.6f}\")\n",
    "# print(f\"Full AutoEncoder - 测试MSE: {test_mse_full:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSDAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加回调函数防止NaN\n",
    "c\n",
    "\n",
    "callbacks = [\n",
    "    TerminateOnNaN(),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)  # 添加梯度裁剪\n",
    "\n",
    "\n",
    " loss_weights={'reconstruction_layer': 1.0, 'target_prediction': 0.1},  # 降低预测损失权重\n",
    "    optimizer=optimizer,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GSDAE训练\n",
    "# optimizer = optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)  # 添加梯度裁剪\n",
    "\n",
    "# GSDAE.compile(\n",
    "#     loss={'reconstruction_layer': 'mean_squared_error', 'target_prediction': 'mean_squared_error'},\n",
    "#     loss_weights={'reconstruction_layer': 1.0, 'target_prediction': 0.1},  # 降低预测损失权重\n",
    "#     optimizer=optimizer,\n",
    "#     metrics=['mae']\n",
    "# )\n",
    "\n",
    "# # 准备训练数据\n",
    "# train_outputs = {'reconstruction_layer': X_train_scal, 'target_prediction': y_train_scal}\n",
    "# test_outputs = {'reconstruction_layer': X_test_scal, 'target_prediction': y_test_scal}\n",
    "\n",
    "# callbacks = [\n",
    "#     TerminateOnNaN(),\n",
    "#     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=1e-6, verbose=1)\n",
    "# ]\n",
    "\n",
    "# # 训练模型\n",
    "# history_gsdae = GSDAE.fit(\n",
    "#     X_train_scal, train_outputs,\n",
    "#     epochs=200,  # 减少训练轮数\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     validation_data=(X_test_scal, test_outputs),\n",
    "#     callbacks=callbacks,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# print(\"GSDAE训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入新的单元格启动自动调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 08s]\n",
      "val_target_prediction_mae: 0.4989844858646393\n",
      "\n",
      "Best val_target_prediction_mae So Far: 0.3744482696056366\n",
      "Total elapsed time: 00h 02m 01s\n",
      "\n",
      "搜索完成！\n",
      "最佳超参数组合:\n",
      " - learning_rate: 0.001\n",
      " - group_lasso_rate: 0.00017125587605581677\n",
      " - l1_rate: 4.500801417535109e-06\n",
      " - prediction_weight: 0.5\n"
     ]
    }
   ],
   "source": [
    "# 1. 配置Tuner\n",
    "# 定义一个简单、无中文的路径，并使用正斜杠\n",
    "tuner_directory = 'D:/kerastuner_dir'\n",
    "\n",
    "# 手动创建父目录，以增加稳定性\n",
    "os.makedirs(tuner_directory, exist_ok=True)\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model_for_tuning,\n",
    "    objective=kt.Objective(\"val_target_prediction_mae\", direction=\"min\"), # 我们的目标是让验证集上预测任务的MAE最小\n",
    "    max_trials=20,  # 总共尝试20种不同的超参数组合\n",
    "    executions_per_trial=1, # 每组参数跑1次\n",
    "    directory=tuner_directory, # 结果保存在这个文件夹\n",
    "    project_name='gsdae_tuning',\n",
    "    overwrite=True # <--- 添加这个参数，以便覆盖旧的/损坏的运行\n",
    ")\n",
    "\n",
    "# 2. 准备回调函数\n",
    "# EarlyStopping可以在模型性能不再提升时提前停止，节省时间\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "# 3. 开始搜索\n",
    "nbr_batches = 20\n",
    "batch_size = int(np.floor(X_train_scal.shape[0]/nbr_batches))\n",
    "print(f\"使用的批次大小: {batch_size}\")\n",
    "\n",
    "print(\"开始超参数搜索...\")\n",
    "tuner.search(\n",
    "    X_train_scal, \n",
    "    {'reconstruction_layer': X_train_scal, 'target_prediction': y_train_scal},\n",
    "    epochs=100, # 每次试验最多跑100轮\n",
    "    batch_size=batch_size, # 使用你之前定义的batch_size\n",
    "    validation_data=(X_test_scal, {'reconstruction_layer': X_test_scal, 'target_prediction': y_test_scal}),\n",
    "    callbacks=[stop_early, TerminateOnNaN()] # 加入回调\n",
    ")\n",
    "\n",
    "# 4. 输出最佳结果\n",
    "print(\"\\n搜索完成！\")\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"最佳超参数组合:\")\n",
    "print(f\" - learning_rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\" - group_lasso_rate: {best_hps.get('group_lasso_rate')}\")\n",
    "print(f\" - l1_rate: {best_hps.get('l1_rate')}\")\n",
    "print(f\" - prediction_weight: {best_hps.get('prediction_weight')}\")\n",
    "\n",
    "# 5. 获取最佳模型并进行评估\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "# 你可以用这个 best_model 替换掉原来的 GSDAE 模型，然后运行后续的评估和分析代码\n",
    "GSDAE = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 数据质量检查 ===\n",
      "X_train_scal - 是否包含NaN: False\n",
      "X_train_scal - 是否包含inf: False\n",
      "X_train_scal - 最大值: 18.497204\n",
      "X_train_scal - 最小值: -3.374657\n",
      "X_train_scal - 标准差: 1.000000\n",
      "y_train_scal - 是否包含NaN: False\n",
      "y_train_scal - 是否包含inf: False\n",
      "y_train_scal - 最大值: 3.066656\n",
      "y_train_scal - 最小值: -1.250043\n",
      "警告: 发现 5 个异常大的特征值 (>10)\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# 数据质量检查\n",
    "print(\"=== 数据质量检查 ===\")\n",
    "print(f\"X_train_scal - 是否包含NaN: {np.isnan(X_train_scal).any()}\")\n",
    "print(f\"X_train_scal - 是否包含inf: {np.isinf(X_train_scal).any()}\")\n",
    "print(f\"X_train_scal - 最大值: {X_train_scal.max():.6f}\")\n",
    "print(f\"X_train_scal - 最小值: {X_train_scal.min():.6f}\")\n",
    "print(f\"X_train_scal - 标准差: {X_train_scal.std():.6f}\")\n",
    "\n",
    "print(f\"y_train_scal - 是否包含NaN: {np.isnan(y_train_scal).any()}\")\n",
    "print(f\"y_train_scal - 是否包含inf: {np.isinf(y_train_scal).any()}\")\n",
    "print(f\"y_train_scal - 最大值: {y_train_scal.max():.6f}\")\n",
    "print(f\"y_train_scal - 最小值: {y_train_scal.min():.6f}\")\n",
    "\n",
    "# 检查是否有异常大的值\n",
    "extreme_threshold = 10\n",
    "extreme_features = np.abs(X_train_scal) > extreme_threshold\n",
    "if extreme_features.any():\n",
    "    print(f\"警告: 发现 {extreme_features.sum()} 个异常大的特征值 (>10)\")\n",
    "\n",
    "print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "改进的正则化参数:\n",
      "组稀疏正则化率: 0.0001\n",
      "L1正则化率: 1e-05\n"
     ]
    }
   ],
   "source": [
    "# 改进的训练策略 - 解决长期训练NaN问题\n",
    "\n",
    "# 1. 进一步降低正则化参数\n",
    "improved_group_lasso_rate = 1e-4  # 从1e-3降低到1e-4\n",
    "improved_l1_rate = 1e-5          # 从1e-4降低到1e-5\n",
    "\n",
    "print(f\"改进的正则化参数:\")\n",
    "print(f\"组稀疏正则化率: {improved_group_lasso_rate}\")\n",
    "print(f\"L1正则化率: {improved_l1_rate}\")\n",
    "\n",
    "# 2. 创建改进的组选择层\n",
    "class ImprovedGroupSelectiveLayer(keras.layers.Layer):\n",
    "    \"\"\"改进的组选择层 - 更稳定的训练\"\"\"\n",
    "    def __init__(self, feature_groups, group_lasso_rate=0.01, l1_rate=0.001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_groups = feature_groups\n",
    "        self.group_lasso_rate = group_lasso_rate\n",
    "        self.l1_rate = l1_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # 使用更保守的初始化\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\", \n",
    "            shape=(int(input_shape[-1]),),\n",
    "            initializer=initializers.RandomUniform(minval=0.8, maxval=1.0),  # 更保守的范围\n",
    "            constraint=ZeroToOneClip(),\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # 添加小的epsilon避免数值问题\n",
    "        epsilon = 1e-7\n",
    "        weighted_features = tf.multiply(inputs, self.kernel + epsilon)\n",
    "        \n",
    "        # 计算正则化损失时添加更多稳定性检查\n",
    "        group_loss = 0.0\n",
    "        for group_indices in self.feature_groups.values():\n",
    "            if len(group_indices) > 0:\n",
    "                group_weights = tf.gather(self.kernel, group_indices)\n",
    "                # 添加epsilon避免除零\n",
    "                group_l2_norm = tf.norm(group_weights + epsilon, ord=2)\n",
    "                group_loss += group_l2_norm\n",
    "        \n",
    "        l1_loss = tf.reduce_sum(tf.abs(self.kernel + epsilon))\n",
    "        \n",
    "        # 更激进的数值裁剪\n",
    "        group_loss = tf.clip_by_value(group_loss, 0.0, 10.0)  # 从1e6降低到10\n",
    "        l1_loss = tf.clip_by_value(l1_loss, 0.0, 10.0)\n",
    "        \n",
    "        # 添加损失时检查是否为NaN\n",
    "        group_loss = tf.where(tf.math.is_nan(group_loss), 0.0, group_loss)\n",
    "        l1_loss = tf.where(tf.math.is_nan(l1_loss), 0.0, l1_loss)\n",
    "        \n",
    "        self.add_loss(self.group_lasso_rate * group_loss)\n",
    "        self.add_loss(self.l1_rate * l1_loss)\n",
    "        \n",
    "        return weighted_features\n",
    "\n",
    "# 3. 改进的模型构建函数\n",
    "def build_improved_GSDAE(input_shape, target_dim, feature_groups, \n",
    "                        nbr_hidden_layers=3, hidden_layer_shape=12, \n",
    "                        encodings_nbr=6, activation=\"relu\", \n",
    "                        group_lasso_rate=0.01, l1_rate=0.001):\n",
    "    \"\"\"构建改进的GSDAE模型\"\"\"\n",
    "    \n",
    "    # 输入层\n",
    "    feature_inputs = Input(shape=[input_shape], name='input')\n",
    "    \n",
    "    # 改进的组选择层\n",
    "    group_selective_layer = ImprovedGroupSelectiveLayer(\n",
    "        feature_groups=feature_groups,\n",
    "        group_lasso_rate=group_lasso_rate,\n",
    "        l1_rate=l1_rate,\n",
    "        name='improved_group_selective_layer'\n",
    "    )\n",
    "    selected_features = group_selective_layer(feature_inputs)\n",
    "    \n",
    "    # 编码器 - 使用LeakyReLU提高稳定性\n",
    "    encoder_full = feature_inputs\n",
    "    for i in range(nbr_hidden_layers):\n",
    "        encoder_full = Dense(hidden_layer_shape, activation='linear', name=f'encoder_hidden_layer_full_{i}')(encoder_full)\n",
    "        encoder_full = keras.layers.LeakyReLU(alpha=0.01)(encoder_full)\n",
    "        encoder_full = keras.layers.BatchNormalization()(encoder_full)  # 添加BatchNorm\n",
    "    \n",
    "    encoder_select = selected_features\n",
    "    for i in range(nbr_hidden_layers):\n",
    "        encoder_select = Dense(hidden_layer_shape, activation='linear', name=f'encoder_hidden_layer_select_{i}')(encoder_select)\n",
    "        encoder_select = keras.layers.LeakyReLU(alpha=0.01)(encoder_select)\n",
    "        encoder_select = keras.layers.BatchNormalization()(encoder_select)\n",
    "    \n",
    "    # 编码层\n",
    "    encoding_full = Dense(encodings_nbr, activation='linear', name='encoding_layer_full')(encoder_full)\n",
    "    encoding_full = keras.layers.LeakyReLU(alpha=0.01)(encoding_full)\n",
    "    \n",
    "    encoding_select = Dense(encodings_nbr, activation='linear', name='encoding_layer_select')(encoder_select)\n",
    "    encoding_select = keras.layers.LeakyReLU(alpha=0.01)(encoding_select)\n",
    "    \n",
    "    # 预测头\n",
    "    prediction_head = Dense(32, activation='linear', name='pred_hidden')(encoding_select)\n",
    "    prediction_head = keras.layers.LeakyReLU(alpha=0.01)(prediction_head)\n",
    "    prediction_head = keras.layers.BatchNormalization()(prediction_head)\n",
    "    target_prediction = Dense(target_dim, activation='linear', name='target_prediction')(prediction_head)\n",
    "    \n",
    "    # 解码器\n",
    "    decoder_layers = []\n",
    "    for i in range(nbr_hidden_layers):\n",
    "        decoder_layer = Dense(hidden_layer_shape, activation='linear', name=f'decoder_hidden_layer_{i}')\n",
    "        decoder_layers.append(decoder_layer)\n",
    "    \n",
    "    reconstruction_layer = Dense(input_shape, activation='linear', name='reconstruction_layer')\n",
    "    \n",
    "    # 应用解码器\n",
    "    decoder_full = encoding_full\n",
    "    decoder_select = encoding_select\n",
    "    \n",
    "    for i, decoder_layer in enumerate(decoder_layers):\n",
    "        decoder_full = decoder_layer(decoder_full)\n",
    "        decoder_full = keras.layers.LeakyReLU(alpha=0.01)(decoder_full)\n",
    "        decoder_full = keras.layers.BatchNormalization()(decoder_full)\n",
    "        \n",
    "        decoder_select = decoder_layer(decoder_select)\n",
    "        decoder_select = keras.layers.LeakyReLU(alpha=0.01)(decoder_select)\n",
    "        decoder_select = keras.layers.BatchNormalization()(decoder_select)\n",
    "    \n",
    "    # 重构输出\n",
    "    reconstruction_full = reconstruction_layer(decoder_full)\n",
    "    reconstruction_select = reconstruction_layer(decoder_select)\n",
    "    \n",
    "    # 构建模型\n",
    "    gsdae_model = Model(inputs=feature_inputs, outputs=[reconstruction_select, target_prediction], name='ImprovedGSDAE')\n",
    "    full_autoencoder = Model(inputs=feature_inputs, outputs=reconstruction_full, name='ImprovedFullAutoEncoder')\n",
    "    feature_selector = Model(inputs=feature_inputs, outputs=selected_features, name='ImprovedFeatureSelector')\n",
    "    \n",
    "    return gsdae_model, full_autoencoder, feature_selector, group_selective_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建改进的GSDAE模型...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nbr_hidden_layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 4. 构建改进的模型\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m构建改进的GSDAE模型...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m ImprovedGSDAE, ImprovedFullAutoEncoder, ImprovedFeatureSelector, ImprovedGroupSelectiveLayer = build_improved_GSDAE(\n\u001b[32m      4\u001b[39m     input_shape=X_train_scal.shape[\u001b[32m1\u001b[39m],\n\u001b[32m      5\u001b[39m     target_dim=y_train_scal.shape[\u001b[32m1\u001b[39m],\n\u001b[32m      6\u001b[39m     feature_groups=feature_groups,\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     nbr_hidden_layers=\u001b[43mnbr_hidden_layers\u001b[49m,\n\u001b[32m      8\u001b[39m     hidden_layer_shape=hidden_layer_shape,\n\u001b[32m      9\u001b[39m     encodings_nbr=encodings_nbr,\n\u001b[32m     10\u001b[39m     group_lasso_rate=improved_group_lasso_rate,\n\u001b[32m     11\u001b[39m     l1_rate=improved_l1_rate\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 5. 改进的训练配置\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m配置改进的训练参数...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'nbr_hidden_layers' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. 构建改进的模型\n",
    "print(\"构建改进的GSDAE模型...\")\n",
    "ImprovedGSDAE, ImprovedFullAutoEncoder, ImprovedFeatureSelector, ImprovedGroupSelectiveLayer = build_improved_GSDAE(\n",
    "    input_shape=X_train_scal.shape[1],\n",
    "    target_dim=y_train_scal.shape[1],\n",
    "    feature_groups=feature_groups,\n",
    "    nbr_hidden_layers=nbr_hidden_layers,\n",
    "    hidden_layer_shape=hidden_layer_shape,\n",
    "    encodings_nbr=encodings_nbr,\n",
    "    group_lasso_rate=improved_group_lasso_rate,\n",
    "    l1_rate=improved_l1_rate\n",
    ")\n",
    "\n",
    "# 5. 改进的训练配置\n",
    "print(\"配置改进的训练参数...\")\n",
    "\n",
    "# 使用更激进的学习率衰减\n",
    "initial_learning_rate = 1e-3\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# 改进的优化器配置\n",
    "improved_optimizer = optimizers.Adam(\n",
    "    learning_rate=lr_schedule,\n",
    "    clipnorm=0.5,  # 更激进的梯度裁剪\n",
    "    clipvalue=0.5   # 添加值裁剪\n",
    ")\n",
    "\n",
    "# 编译改进的模型\n",
    "ImprovedGSDAE.compile(\n",
    "    loss={'reconstruction_layer': 'mean_squared_error', 'target_prediction': 'mean_squared_error'},\n",
    "    loss_weights={'reconstruction_layer': 0.8, 'target_prediction': 0.2},  # 调整权重比例\n",
    "    optimizer=improved_optimizer,\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# 6. 改进的回调函数\n",
    "improved_callbacks = [\n",
    "    TerminateOnNaN(),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),\n",
    "    # 添加自定义回调监控训练过程\n",
    "    keras.callbacks.LambdaCallback(\n",
    "        on_epoch_end=lambda epoch, logs: print(f\"Epoch {epoch}: loss={logs.get('loss', 0):.6f}, val_loss={logs.get('val_loss', 0):.6f}\")\n",
    "        if epoch % 10 == 0 else None\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"改进的模型配置完成！\")\n",
    "print(\"主要改进:\")\n",
    "print(\"1. 更低的正则化参数\")\n",
    "print(\"2. 更保守的权重初始化(0.8-1.0)\")\n",
    "print(\"3. LeakyReLU + BatchNormalization\")\n",
    "print(\"4. 指数学习率衰减\")\n",
    "print(\"5. 更激进的梯度裁剪\")\n",
    "print(\"6. 调整的损失权重\")\n",
    "print(\"7. 更多数值稳定性检查\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 稳定的长期训练测试\n",
    "print(\"开始改进的GSDAE训练...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 准备训练数据\n",
    "train_outputs = {'reconstruction_layer': X_train_scal, 'target_prediction': y_train_scal}\n",
    "test_outputs = {'reconstruction_layer': X_test_scal, 'target_prediction': y_test_scal}\n",
    "\n",
    "try:\n",
    "    # 训练改进的模型 - 现在可以使用更多轮数\n",
    "    history_improved = ImprovedGSDAE.fit(\n",
    "        X_train_scal, train_outputs,\n",
    "        epochs=100,  # 现在可以安全地使用更多轮数\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        validation_data=(X_test_scal, test_outputs),\n",
    "        callbacks=improved_callbacks,\n",
    "        verbose=0  # 减少输出，使用回调函数监控\n",
    "    )\n",
    "    \n",
    "    print(\"✅ 改进的GSDAE训练成功完成！\")\n",
    "    print(f\"训练轮数: {len(history_improved.history['loss'])}\")\n",
    "    print(f\"最终训练损失: {history_improved.history['loss'][-1]:.6f}\")\n",
    "    print(f\"最终验证损失: {history_improved.history['val_loss'][-1]:.6f}\")\n",
    "    \n",
    "    # 检查是否存在NaN\n",
    "    final_loss = history_improved.history['loss'][-1]\n",
    "    final_val_loss = history_improved.history['val_loss'][-1]\n",
    "    \n",
    "    if np.isnan(final_loss) or np.isnan(final_val_loss):\n",
    "        print(\"❌ 仍然出现NaN，需要进一步调整参数\")\n",
    "    else:\n",
    "        print(\"✅ 训练过程稳定，无NaN问题\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 训练过程中出现错误: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN问题诊断工具\n",
    "def diagnose_nan_issue(model, X_sample, y_sample):\n",
    "    \"\"\"诊断模型中的NaN问题\"\"\"\n",
    "    print(\"=== NaN问题诊断 ===\")\n",
    "    \n",
    "    # 1. 检查模型权重\n",
    "    print(\"1. 检查模型权重:\")\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel') and layer.kernel is not None:\n",
    "            weights = layer.kernel.numpy()\n",
    "            if np.isnan(weights).any():\n",
    "                print(f\"   ❌ {layer.name}: 发现NaN权重\")\n",
    "            elif np.isinf(weights).any():\n",
    "                print(f\"   ⚠️  {layer.name}: 发现无穷大权重\")\n",
    "            else:\n",
    "                weight_max = np.max(np.abs(weights))\n",
    "                print(f\"   ✅ {layer.name}: 权重正常 (最大绝对值: {weight_max:.6f})\")\n",
    "    \n",
    "    # 2. 检查前向传播\n",
    "    print(\"\\n2. 检查前向传播:\")\n",
    "    try:\n",
    "        outputs = model.predict(X_sample[:5], verbose=0)\n",
    "        if isinstance(outputs, list):\n",
    "            for i, output in enumerate(outputs):\n",
    "                if np.isnan(output).any():\n",
    "                    print(f\"   ❌ 输出{i}: 包含NaN\")\n",
    "                elif np.isinf(output).any():\n",
    "                    print(f\"   ⚠️  输出{i}: 包含无穷大\")\n",
    "                else:\n",
    "                    print(f\"   ✅ 输出{i}: 正常\")\n",
    "        else:\n",
    "            if np.isnan(outputs).any():\n",
    "                print(\"   ❌ 输出: 包含NaN\")\n",
    "            elif np.isinf(outputs).any():\n",
    "                print(\"   ⚠️  输出: 包含无穷大\")\n",
    "            else:\n",
    "                print(\"   ✅ 输出: 正常\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ 前向传播失败: {e}\")\n",
    "    \n",
    "    # 3. 检查损失计算\n",
    "    print(\"\\n3. 检查损失计算:\")\n",
    "    try:\n",
    "        loss = model.evaluate(X_sample[:5], {'reconstruction_layer': X_sample[:5], \n",
    "                                           'target_prediction': y_sample[:5]}, verbose=0)\n",
    "        if np.isnan(loss[0]):\n",
    "            print(f\"   ❌ 总损失: NaN\")\n",
    "        else:\n",
    "            print(f\"   ✅ 总损失: {loss[0]:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ 损失计算失败: {e}\")\n",
    "    \n",
    "    print(\"=\" * 30)\n",
    "\n",
    "# 运行原始模型的诊断（如果已训练）\n",
    "if 'GSDAE' in locals() and 'history_gsdae' in locals():\n",
    "    print(\"诊断原始GSDAE模型:\")\n",
    "    diagnose_nan_issue(GSDAE, X_train_scal, y_train_scal)\n",
    "\n",
    "# 快速训练测试 - 逐步增加轮数观察何时出现NaN\n",
    "def gradual_training_test(model, X_train, y_train, X_val, y_val, max_epochs=50):\n",
    "    \"\"\"逐步增加训练轮数，观察NaN出现的时机\"\"\"\n",
    "    print(\"=== 逐步训练测试 ===\")\n",
    "    \n",
    "    train_outputs = {'reconstruction_layer': X_train, 'target_prediction': y_train}\n",
    "    val_outputs = {'reconstruction_layer': X_val, 'target_prediction': y_val}\n",
    "    \n",
    "    step_sizes = [5, 10, 20, 30, 50]\n",
    "    \n",
    "    for epochs in step_sizes:\n",
    "        if epochs > max_epochs:\n",
    "            break\n",
    "            \n",
    "        print(f\"\\n测试 {epochs} 轮训练:\")\n",
    "        try:\n",
    "            # 重新编译模型确保干净状态\n",
    "            model.compile(\n",
    "                loss={'reconstruction_layer': 'mean_squared_error', 'target_prediction': 'mean_squared_error'},\n",
    "                loss_weights={'reconstruction_layer': 1.0, 'target_prediction': 0.1},\n",
    "                optimizer=optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n",
    "                metrics=['mae']\n",
    "            )\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train, train_outputs,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(X_val, val_outputs),\n",
    "                callbacks=[TerminateOnNaN()],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            final_loss = history.history['loss'][-1]\n",
    "            final_val_loss = history.history['val_loss'][-1]\n",
    "            \n",
    "            if np.isnan(final_loss) or np.isnan(final_val_loss):\n",
    "                print(f\"   ❌ {epochs}轮后出现NaN - 临界点找到！\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"   ✅ {epochs}轮训练正常 (loss: {final_loss:.6f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ {epochs}轮训练失败: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(\"=\" * 30)\n",
    "\n",
    "print(\"可以使用以下函数进行诊断:\")\n",
    "print(\"1. diagnose_nan_issue(model, X_sample, y_sample) - 诊断当前模型状态\")\n",
    "print(\"2. gradual_training_test(model, X_train, y_train, X_val, y_val) - 找到NaN出现的临界点\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估GSDAE模型\n",
    "train_pred_gsdae = GSDAE.predict(X_train_scal)\n",
    "test_pred_gsdae = GSDAE.predict(X_test_scal)\n",
    "\n",
    "train_recon_gsdae, train_target_pred = train_pred_gsdae\n",
    "test_recon_gsdae, test_target_pred = test_pred_gsdae\n",
    "\n",
    "# 计算误差\n",
    "train_mse_recon = mean_squared_error(X_train_scal, train_recon_gsdae)\n",
    "test_mse_recon = mean_squared_error(X_test_scal, test_recon_gsdae)\n",
    "train_mse_pred = mean_squared_error(y_train_scal, train_target_pred)\n",
    "test_mse_pred = mean_squared_error(y_test_scal, test_target_pred)\n",
    "\n",
    "# 新增：MAE（标准化尺度）\n",
    "train_mae_recon = mean_absolute_error(X_train_scal, train_recon_gsdae)\n",
    "test_mae_recon  = mean_absolute_error(X_test_scal, test_recon_gsdae)\n",
    "train_mae_pred  = mean_absolute_error(y_train_scal, train_target_pred)\n",
    "test_mae_pred   = mean_absolute_error(y_test_scal, test_target_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 反标准化计算R²\n",
    "train_target_pred_orig = scaler_y.inverse_transform(train_target_pred)\n",
    "test_target_pred_orig = scaler_y.inverse_transform(test_target_pred)\n",
    "y_train_orig = scaler_y.inverse_transform(y_train_scal)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test_scal)\n",
    "\n",
    "train_r2 = r2_score(y_train_orig, train_target_pred_orig)\n",
    "test_r2 = r2_score(y_test_orig, test_target_pred_orig)\n",
    "\n",
    "# 新增：目标MAE（原单位，便于解读）\n",
    "train_mae_pred_orig = mean_absolute_error(y_train_orig, train_target_pred_orig)\n",
    "test_mae_pred_orig  = mean_absolute_error(y_test_orig,  test_target_pred_orig)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"GSDAE模型评估结果:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"重构任务 - 训练MSE: {train_mse_recon:.6f}\")\n",
    "print(f\"重构任务 - 测试MSE: {test_mse_recon:.6f}\")\n",
    "print(f\"重构任务 - 训练MAE: {train_mae_recon:.6f}\")\n",
    "print(f\"重构任务 - 测试MAE: {test_mae_recon:.6f}\")\n",
    "print(f\"预测任务 - 训练MSE: {train_mse_pred:.6f}\")\n",
    "print(f\"预测任务 - 测试MSE: {test_mse_pred:.6f}\")\n",
    "print(f\"预测任务 - 训练MAE(标准化): {train_mae_pred:.6f}\")\n",
    "print(f\"预测任务 - 测试MAE(标准化): {test_mae_pred:.6f}\")\n",
    "print(f\"目标预测 - 训练MAE(原单位): {train_mae_pred_orig:.6f}\")\n",
    "print(f\"目标预测 - 测试MAE(原单位): {test_mae_pred_orig:.6f}\")\n",
    "print(f\"目标预测 - 训练R²: {train_r2:.4f}\")\n",
    "print(f\"目标预测 - 测试R²: {test_r2:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 特征重要性分析\n",
    "# def analyze_feature_importance(group_selective_layer, feature_groups, feature_names):\n",
    "#     \"\"\"两层重要性分析\"\"\"\n",
    "#     weights = group_selective_layer.kernel.numpy()\n",
    "    \n",
    "#     # 组重要性评估\n",
    "#     group_importance = {}\n",
    "#     for group_name, indices in feature_groups.items():\n",
    "#         if len(indices) > 0:\n",
    "#             group_weights = weights[indices]\n",
    "#             group_l2_norm = np.linalg.norm(group_weights, ord=2)\n",
    "#             group_importance[group_name] = group_l2_norm\n",
    "    \n",
    "#     sorted_groups = sorted(group_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     # 组内关键特征识别\n",
    "#     feature_importance = {}\n",
    "#     for group_name, indices in feature_groups.items():\n",
    "#         if len(indices) > 0:\n",
    "#             group_weights = weights[indices]\n",
    "#             group_features = [feature_names[i] for i in indices]\n",
    "#             feature_weight_pairs = list(zip(group_features, group_weights))\n",
    "#             sorted_features = sorted(feature_weight_pairs, key=lambda x: x[1], reverse=True)\n",
    "#             feature_importance[group_name] = sorted_features\n",
    "    \n",
    "#     return sorted_groups, feature_importance, weights\n",
    "\n",
    "# # 执行重要性分析\n",
    "# sorted_groups, feature_importance, weights = analyze_feature_importance(\n",
    "#     GroupSelectiveLayer, feature_groups, feature_names\n",
    "# )\n",
    "\n",
    "# print(\"特征组重要性排名:\")\n",
    "# for i, (group_name, importance) in enumerate(sorted_groups):\n",
    "#     print(f\"{i+1:2d}. {group_name:12s}: {importance:.4f}\")\n",
    "\n",
    "# print(\"\\n各组内关键特征 (前3个):\")\n",
    "# for group_name, features in feature_importance.items():\n",
    "#     print(f\"\\n{group_name}:\")\n",
    "#     for i, (feat_name, weight) in enumerate(features[:3]):\n",
    "#         print(f\"  {i+1}. {feat_name}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "def analyze_feature_importance(group_selective_layer, feature_groups, feature_names, \n",
    "                               metric='l2', normalize=True, top_k=None):\n",
    "    \"\"\"两层重要性分析；metric: 'l2'|'l1'|'max'|'mean'；normalize=True时做组大小归一化\"\"\"\n",
    "    weights = group_selective_layer.kernel.numpy()\n",
    "\n",
    "    # 组重要性评估\n",
    "    group_importance = {}\n",
    "    for group_name, indices in feature_groups.items():\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        gw = weights[indices]\n",
    "\n",
    "        # 可选：仅取组内Top-K权重再评估\n",
    "        if top_k is not None and top_k < len(gw):\n",
    "            gw = np.sort(gw)[-top_k:]\n",
    "\n",
    "        if metric == 'l2':\n",
    "            score = np.linalg.norm(gw, ord=2)\n",
    "            if normalize:\n",
    "                score = score / np.sqrt(len(gw))   # RMS：消除组大小影响\n",
    "        elif metric == 'l1':\n",
    "            score = np.sum(np.abs(gw))\n",
    "            if normalize:\n",
    "                score = score / len(gw)           # 平均绝对权重\n",
    "        elif metric == 'max':\n",
    "            score = float(np.max(gw))\n",
    "        else:  # 'mean'\n",
    "            score = float(np.mean(gw))\n",
    "        group_importance[group_name] = float(score)\n",
    "\n",
    "    sorted_groups = sorted(group_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 组内关键特征识别（保持原逻辑）\n",
    "    feature_importance = {}\n",
    "    for group_name, indices in feature_groups.items():\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        gw = weights[indices]\n",
    "        gf = [feature_names[i] for i in indices]\n",
    "        feature_importance[group_name] = sorted(zip(gf, gw), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_groups, feature_importance, weights\n",
    "\n",
    "# 调用示例：用RMS做组评分\n",
    "sorted_groups, feature_importance, weights = analyze_feature_importance(\n",
    "    GroupSelectiveLayer, feature_groups, feature_names,\n",
    "    metric='l2', normalize=True  # 或 metric='l1', normalize=True 为L1平均\n",
    ")\n",
    "\n",
    "\n",
    "print(\"特征组重要性排名:\")\n",
    "for i, (group_name, importance) in enumerate(sorted_groups):\n",
    "    print(f\"{i+1:2d}. {group_name:12s}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\n各组内关键特征 (前3个):\")\n",
    "for group_name, features in feature_importance.items():\n",
    "    print(f\"\\n{group_name}:\")\n",
    "    for i, (feat_name, weight) in enumerate(features[:3]):\n",
    "        print(f\"  {i+1}. {feat_name}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化结果\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# 组重要性\n",
    "top_n_groups = min(8, len(sorted_groups))\n",
    "groups = [item[0] for item in sorted_groups[:top_n_groups]]\n",
    "importance = [item[1] for item in sorted_groups[:top_n_groups]]\n",
    "\n",
    "axes[0, 0].barh(groups, importance, color='skyblue')\n",
    "axes[0, 0].set_xlabel('组重要性 (L2范数)')\n",
    "axes[0, 0].set_title('特征组重要性排名')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 最重要组的关键特征\n",
    "if sorted_groups:\n",
    "    top_group = sorted_groups[0][0]\n",
    "    top_features = feature_importance[top_group][:5]\n",
    "    \n",
    "    feature_names_short = [item[0][:15] + '...' if len(item[0]) > 15 else item[0] for item in top_features]\n",
    "    feature_weights = [item[1] for item in top_features]\n",
    "    \n",
    "    axes[0, 1].barh(feature_names_short, feature_weights, color='lightcoral')\n",
    "    axes[0, 1].set_xlabel('特征权重')\n",
    "    axes[0, 1].set_title(f'\"{top_group}\"组内关键特征')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 训练损失\n",
    "axes[1, 0].plot(history_gsdae.history['loss'], label='训练损失')\n",
    "axes[1, 0].plot(history_gsdae.history['val_loss'], label='验证损失')\n",
    "axes[1, 0].set_title('训练损失')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 预测准确性\n",
    "axes[1, 1].scatter(y_test_orig, test_target_pred_orig, alpha=0.6)\n",
    "axes[1, 1].plot([y_test_orig.min(), y_test_orig.max()], [y_test_orig.min(), y_test_orig.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_xlabel('实际值')\n",
    "axes[1, 1].set_ylabel('预测值')\n",
    "axes[1, 1].set_title(f'预测 vs 实际 (R² = {test_r2:.3f})')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\\n\n",
    "\\n\n",
    "### GSDAE模型总结\\n\n",
    "\\n\n",
    "本notebook实现了基于方案文档的GSDAE模型，主要改进包括：\\n\n",
    "\\n\n",
    "1. **组稀疏正则化**: 针对生态因子的天然分组结构\\n\n",
    "2. **半监督学习**: 引入丹参酮含量作为监督信号\\n\n",
    "3. **预测头**: 专门的预测分支用于药效成分含量预测\\n\n",
    "4. **复合损失函数**: 重构误差 + 预测误差 + 组稀疏正则化\\n\n",
    "5. **两层重要性分析**: 组重要性评估 + 组内关键特征识别\\n\n",
    "\\n\n",
    "### 模型优势\\n\n",
    "\\n\n",
    "- **生物学意义**: 特征分组符合生态因子的天然结构\\n\n",
    "- **目标导向**: 半监督学习机制与药效预测任务对齐\\n\n",
    "- **可解释性**: 提供宏观到微观的多层次分析结果\\n\n",
    "- **实用性**: 同时完成特征选择、降维和预测任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 绘制图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化：各特征组 Top-10 特征权重分布面板（类似图1）\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "# 设置中文字体显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def _get_layer_kernel(layer):\n",
    "    if hasattr(layer, \"kernel\") and layer.kernel is not None:\n",
    "        w = layer.kernel.numpy()\n",
    "        return np.asarray(w, dtype=float)\n",
    "    raise ValueError(\"layer 不含 kernel 权重，请确认传入的是 GroupSelectiveLayer/ImprovedGroupSelectiveLayer 的实例。\")\n",
    "\n",
    "def compute_group_feature_weights(layer, feature_groups, feature_names):\n",
    "    \"\"\"返回 {组名: [(特征名, 权重), ...]}\"\"\"\n",
    "    weights = _get_layer_kernel(layer)\n",
    "    result = {}\n",
    "    for g, idx in feature_groups.items():\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        pairs = [(feature_names[i], float(weights[i])) for i in idx]\n",
    "        # 只保留非负权重（该层已约束到[0,1]，此处为稳健性）\n",
    "        pairs = [(n, max(0.0, w)) for n, w in pairs]\n",
    "        result[g] = pairs\n",
    "    return result\n",
    "\n",
    "def plot_group_weight_panels(group_feature_weights, top_k=10, group_order=None, ncols=4):\n",
    "    \"\"\"绘制各组Top-K特征权重面板\"\"\"\n",
    "    # 组显示顺序\n",
    "    keys = list(group_feature_weights.keys())\n",
    "    if group_order:\n",
    "        ordered = [g for g in group_order if g in group_feature_weights] + [g for g in keys if g not in group_order]\n",
    "    else:\n",
    "        ordered = keys\n",
    "\n",
    "    ordered_map = OrderedDict((g, group_feature_weights[g]) for g in ordered)\n",
    "    ng = len(ordered_map)\n",
    "    ncols = min(ncols, max(1, ng))\n",
    "    nrows = int(np.ceil(ng / ncols))\n",
    "\n",
    "    # 统一x轴上限，便于对比\n",
    "    global_max = 0.0\n",
    "    for feats in ordered_map.values():\n",
    "        if feats:\n",
    "            global_max = max(global_max, max(w for _, w in feats))\n",
    "    global_max = max(global_max, 1e-6) * 1.05\n",
    "\n",
    "    # 颜色：每组一个主色\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=max(10, ng))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4.6*ncols+0.6, 3.2*nrows+1), squeeze=False, sharex=True)\n",
    "    ax_list = axes.flatten()\n",
    "\n",
    "    # 先关掉所有坐标轴，避免空格子显示框线\n",
    "    for ax in ax_list:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for i, (g, feats) in enumerate(ordered_map.items()):\n",
    "        ax = ax_list[i]\n",
    "        ax.axis(\"on\")\n",
    "        feats_sorted = sorted(feats, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        # 反向画水平条，便于从上到下显示从小到大\n",
    "        names = [n if len(n) <= 18 else n[:18] + \"…\" for n, _ in feats_sorted][::-1]\n",
    "        vals = [v for _, v in feats_sorted][::-1]\n",
    "\n",
    "        ax.barh(names, vals, color=palette[i % len(palette)], alpha=0.9)\n",
    "        ax.set_title(g, fontsize=12, pad=6)\n",
    "        ax.set_xlim(0, global_max)\n",
    "        ax.grid(True, axis='x', alpha=0.3)\n",
    "        ax.tick_params(axis='y', labelsize=9)\n",
    "\n",
    "        # 数值标注\n",
    "        for y, v in enumerate(vals):\n",
    "            ax.text(v, y, f\" {v:.3f}\", va='center', ha='left', fontsize=8)\n",
    "\n",
    "    # 总标题与公共x轴标签\n",
    "    fig.suptitle(\"SDAE/GSDAE 各特征组权重分布（每组Top-10）\", fontsize=14)\n",
    "    fig.text(0.5, 0.01, \"权重值\", ha='center', fontsize=11)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# 使用示例（训练完成后运行）\n",
    "# 自动选择已存在的组选择层实例\n",
    "layer_inst = None\n",
    "for name in [\"ImprovedGroupSelectiveLayer\", \"GroupSelectiveLayer\"]:\n",
    "    if name in globals():\n",
    "        layer_inst = globals()[name]\n",
    "        break\n",
    "if layer_inst is None:\n",
    "    raise RuntimeError(\"未找到组选择层实例，请先训练模型或检查变量名。\")\n",
    "\n",
    "group_feature_weights = compute_group_feature_weights(layer_inst, feature_groups, feature_names)\n",
    "\n",
    "# 推荐的组显示顺序（若某组不存在会自动跳过）\n",
    "preferred_order = [\"土壤元素\", \"作物元素\", \"土壤养分\", \"地理信息\", \"气候环境\",\n",
    "                   \"省份\", \"城市\", \"地貌\", \"土壤类型\", \"栽培类型\", \"气候类型\"]\n",
    "\n",
    "plot_group_weight_panels(group_feature_weights, top_k=10, group_order=preferred_order, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文字体与“0权重可见”的权重面板图\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "from matplotlib import font_manager as fm\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "def set_cn_font():\n",
    "    \"\"\"自动注册并启用中文字体\"\"\"\n",
    "    candidates = [\n",
    "        r\"C:\\Windows\\Fonts\\msyh.ttc\",   # 微软雅黑\n",
    "        r\"C:\\Windows\\Fonts\\msyh.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\simhei.ttf\", # 黑体\n",
    "        r\"C:\\Windows\\Fonts\\simsun.ttc\"  # 宋体\n",
    "    ]\n",
    "    chosen = None\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                fm.fontManager.addfont(p)\n",
    "                chosen = FontProperties(fname=p).get_name()\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if chosen is None:\n",
    "        # 回退到常用名称\n",
    "        chosen = \"Microsoft YaHei\"\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = [chosen, 'SimHei', 'SimSun', 'DejaVu Sans']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    print(f\"已设置中文字体: {chosen}\")\n",
    "\n",
    "set_cn_font()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def _get_layer_kernel(layer):\n",
    "    if hasattr(layer, \"kernel\") and layer.kernel is not None:\n",
    "        w = layer.kernel.numpy()\n",
    "        return np.asarray(w, dtype=float)\n",
    "    raise ValueError(\"layer 不含 kernel 权重，请确认传入的是 GroupSelectiveLayer/ImprovedGroupSelectiveLayer 的实例。\")\n",
    "\n",
    "def compute_group_feature_weights(layer, feature_groups, feature_names):\n",
    "    \"\"\"返回 {组名: [(特征名, 权重), ...]}（包含0值）\"\"\"\n",
    "    weights = _get_layer_kernel(layer)\n",
    "    result = {}\n",
    "    for g, idx in feature_groups.items():\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        pairs = [(feature_names[i], float(max(0.0, weights[i]))) for i in idx]  # 负值截断为0，保留0\n",
    "        result[g] = pairs\n",
    "    return result\n",
    "\n",
    "def plot_group_weight_panels(group_feature_weights, top_k=None, group_order=None, ncols=4, zero_min_ratio=0.02):\n",
    "    \"\"\"\n",
    "    绘制各组Top-K特征权重面板\n",
    "    - top_k=None 显示组内全部特征\n",
    "    - zero_min_ratio: 0值条形显示为 (global_max * ratio) 的最小宽度\n",
    "    \"\"\"\n",
    "    # 组显示顺序\n",
    "    keys = list(group_feature_weights.keys())\n",
    "    if group_order:\n",
    "        ordered = [g for g in group_order if g in group_feature_weights] + [g for g in keys if g not in group_order]\n",
    "    else:\n",
    "        ordered = keys\n",
    "\n",
    "    ordered_map = OrderedDict((g, group_feature_weights[g]) for g in ordered)\n",
    "    ng = len(ordered_map)\n",
    "    ncols = min(ncols, max(1, ng))\n",
    "    nrows = int(np.ceil(ng / ncols))\n",
    "\n",
    "    # 统一x轴上限，便于对比（按真实权重计算）\n",
    "    all_vals = [w for feats in ordered_map.values() for _, w in feats] or [0.0]\n",
    "    global_max = float(max(all_vals))\n",
    "    if global_max <= 0:\n",
    "        global_max = 1.0  # 全0时的可视上限\n",
    "    xlim = global_max * 1.05\n",
    "    min_bar = max(global_max * zero_min_ratio, 1e-6)  # 0值条的最小显示宽度\n",
    "\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=max(10, ng))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4.8*ncols+0.6, 3.2*nrows+1), squeeze=False, sharex=True)\n",
    "    ax_list = axes.flatten()\n",
    "\n",
    "    # 先关掉所有坐标轴，避免空格子显示框线\n",
    "    for ax in ax_list:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for i, (g, feats) in enumerate(ordered_map.items()):\n",
    "        ax = ax_list[i]\n",
    "        ax.axis(\"on\")\n",
    "        feats_sorted = sorted(feats, key=lambda x: x[1], reverse=True)\n",
    "        if top_k is not None:\n",
    "            feats_sorted = feats_sorted[:top_k]\n",
    "\n",
    "        names = [n if len(n) <= 18 else n[:18] + \"…\" for n, _ in feats_sorted][::-1]\n",
    "        vals_true = [v for _, v in feats_sorted][::-1]\n",
    "        # 0值条形使用最小宽度渲染，但保留真实数值标注\n",
    "        vals_plot = [v if v > 0 else min_bar for v in vals_true]\n",
    "\n",
    "        ax.barh(names, vals_plot, color=palette[i % len(palette)], alpha=0.9)\n",
    "        ax.set_title(g, fontsize=12, pad=6)\n",
    "        ax.set_xlim(0, xlim)\n",
    "        ax.grid(True, axis='x', alpha=0.3)\n",
    "        ax.tick_params(axis='y', labelsize=9)\n",
    "\n",
    "        # 数值标注（使用真实值；位置用绘制宽度以保证可见）\n",
    "        for y, (vp, vt) in enumerate(zip(vals_plot, vals_true)):\n",
    "            ax.text(vp, y, f\" {vt:.3f}\", va='center', ha='left', fontsize=8)\n",
    "\n",
    "    fig.suptitle(\"SDAE/GSDAE 各特征组权重分布（含0值；每组Top-K）\", fontsize=14)\n",
    "    fig.text(0.5, 0.01, \"权重值\", ha='center', fontsize=11)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# 选择已存在的组选择层实例\n",
    "layer_inst = None\n",
    "for name in [\"ImprovedGroupSelectiveLayer\", \"GroupSelectiveLayer\"]:\n",
    "    if name in globals():\n",
    "        layer_inst = globals()[name]\n",
    "        break\n",
    "if layer_inst is None:\n",
    "    raise RuntimeError(\"未找到组选择层实例，请先训练模型或检查变量名。\")\n",
    "\n",
    "group_feature_weights = compute_group_feature_weights(layer_inst, feature_groups, feature_names)\n",
    "\n",
    "# 推荐顺序；top_k=None 显示组内全部特征（包含0）\n",
    "preferred_order = [\"土壤元素\", \"作物元素\", \"土壤养分\", \"地理信息\", \"气候环境\",\n",
    "                   \"省份\", \"城市\", \"地貌\", \"土壤类型\", \"栽培类型\", \"气候类型\"]\n",
    "\n",
    "plot_group_weight_panels(group_feature_weights, top_k=None, group_order=preferred_order, ncols=4, zero_min_ratio=0.02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GSDAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
