{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSDAE (Group Selective Deep AutoEncoder) for Danshen Analysis\n",
    "\n",
    "基于方案文档的改进版本，包含：\n",
    "1. 组稀疏正则化 (Group Lasso)\n",
    "2. 半监督学习机制\n",
    "3. 预测头 (Prediction Head)\n",
    "4. 复合损失函数\n",
    "5. 两层重要性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import glob\n",
    "\n",
    "import seaborn as sns  # 新增\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras import optimizers, initializers\n",
    "from tensorflow.keras.callbacks import TerminateOnNaN, ReduceLROnPlateau\n",
    "\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到了 4 个特征文件。\n",
      "目标变量 'SumTS' 已从 'climate.csv' 中分离。\n",
      "\n",
      "合并后的特征数据形状: (991, 69)\n",
      "对齐后的目标变量形状: (991, 1)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 定义特征文件所在的目录 ---\n",
    "feature_files_dir = './data_R/EnvironmentGroup/'\n",
    "sample_id_col = 'SampleN' # <--- 假设每个文件里都有这个样本ID列\n",
    "\n",
    "# --- 2. 查找所有特征文件 ---\n",
    "csv_files = glob.glob(os.path.join(feature_files_dir, '*.csv'))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"在目录 '{feature_files_dir}' 中没有找到任何CSV文件。\")\n",
    "\n",
    "print(f\"找到了 {len(csv_files)} 个特征文件。\")\n",
    "\n",
    "# --- 3. 从第一个文件中获取目标变量 ---\n",
    "try:\n",
    "    first_df = pd.read_csv(csv_files[0])\n",
    "    # 假设目标变量是最后一列\n",
    "    target_column_name = first_df.columns[-1]\n",
    "    # 提取目标变量和样本ID，并设置索引\n",
    "    main_target = first_df[[sample_id_col, target_column_name]].set_index(sample_id_col)\n",
    "    print(f\"目标变量 '{target_column_name}' 已从 '{os.path.basename(csv_files[0])}' 中分离。\")\n",
    "except (IndexError, KeyError) as e:\n",
    "    raise ValueError(f\"无法从文件 {csv_files[0]} 中提取目标变量或样本ID: {e}\")\n",
    "\n",
    "# --- 4. 循环读取所有文件，合并特征 ---\n",
    "all_feature_dfs = []\n",
    "for file_path in csv_files:\n",
    "    try:\n",
    "        # 读取数据，并将样本ID设为索引\n",
    "        df = pd.read_csv(file_path).set_index(sample_id_col)\n",
    "        # 删除目标列，只保留特征\n",
    "        features_only = df.drop(columns=[target_column_name])\n",
    "        all_feature_dfs.append(features_only)\n",
    "    except (FileNotFoundError, KeyError) as e:\n",
    "        print(f\"警告：处理文件 {file_path} 时出错，已跳过。错误: {e}\")\n",
    "\n",
    "# --- 5. 合并所有特征 ---\n",
    "# 使用 outer join 以保留所有样本，缺失处会用NaN填充\n",
    "# 使用 inner join 只保留所有文件中都存在的样本\n",
    "feature_data = pd.concat(all_feature_dfs, axis=1, join='inner')\n",
    "\n",
    "# --- 6. 对齐特征和目标 ---\n",
    "# 确保 feature_data 和 main_target 的样本索引一致\n",
    "common_samples = feature_data.index.intersection(main_target.index)\n",
    "feature_data = feature_data.loc[common_samples]\n",
    "main_target = main_target.loc[common_samples]\n",
    "\n",
    "print(f\"\\n合并后的特征数据形状: {feature_data.shape}\")\n",
    "print(f\"对齐后的目标变量形状: {main_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理后特征数据形状: (498, 72)\n",
      "预处理后目标变量形状: (498, 1)\n"
     ]
    }
   ],
   "source": [
    "# --- 数据预处理 ---\n",
    "# 1. 处理缺失值：删除特征超过50%缺失的列\n",
    "thresh = len(feature_data) * 0.5\n",
    "feature_data = feature_data.dropna(axis=1, thresh=thresh)\n",
    "\n",
    "# 2. 处理缺失值：删除任何包含缺失值的行（样本）\n",
    "# 首先合并，然后删除带有任何NaN的行，最后再分开，以确保对齐\n",
    "temp_df = pd.concat([feature_data, main_target], axis=1)\n",
    "temp_df.dropna(inplace=True)\n",
    "\n",
    "# 重新分离\n",
    "feature_data = temp_df.drop(columns=main_target.columns)\n",
    "main_target = temp_df[main_target.columns]\n",
    "\n",
    "# 3. 独热编码\n",
    "categorical_columns = [\"Province\", \"City\", \"Microb\", \"Landscape\", \"SoilType\", \"soilclass\", \"CultivationType\", \"ClimateType\", \"按气候聚类划分的类型\"]\n",
    "categorical_columns = [col for col in categorical_columns if col in feature_data.columns]\n",
    "if categorical_columns:\n",
    "    feature_data = pd.get_dummies(feature_data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "print(f\"预处理后特征数据形状: {feature_data.shape}\")\n",
    "print(f\"预处理后目标变量形状: {main_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: (398, 72), 测试集: (100, 72)\n",
      "特征总数: 72\n"
     ]
    }
   ],
   "source": [
    "# 数据分割和标准化\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_data, main_target, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scal = scaler_X.fit_transform(X_train)\n",
    "X_test_scal = scaler_X.transform(X_test)\n",
    "y_train_scal = scaler_y.fit_transform(y_train)\n",
    "y_test_scal = scaler_y.transform(y_test)\n",
    "\n",
    "# --- 新增：提前反标准化 y_test，为后续评估做准备 ---\n",
    "y_test_orig = scaler_y.inverse_transform(y_test_scal)\n",
    "\n",
    "feature_names = list(feature_data.columns)\n",
    "print(f\"训练集: {X_train_scal.shape}, 测试集: {X_test_scal.shape}\")\n",
    "print(f\"特征总数: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建了 4 个特征组:\n",
      "  - climate: 9 个特征\n",
      "  - geo: 6 个特征\n",
      "  - physicochemical: 15 个特征\n",
      "  - soil_element: 45 个特征\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "def _find_indices_by_names(feature_names, names_list):\n",
    "    \"\"\"在 feature_names 中匹配 names_list（支持精确/包含/前缀），返回索引列表（去重、有序）\"\"\"\n",
    "    fn_lower = [f.lower() for f in feature_names]\n",
    "    indices = []\n",
    "    names_lower = [n.lower() for n in names_list]\n",
    "    for i, fname in enumerate(fn_lower):\n",
    "        for nm in names_lower:\n",
    "            if fname == nm or nm in fname or fname.startswith(nm) or fname.endswith(nm):\n",
    "                indices.append(i)\n",
    "                break\n",
    "    return sorted(set(indices))\n",
    "\n",
    "def create_feature_groups(feature_names, method='external', external_dir='./data_R/EnvironmentGroup', custom_groups=None):\n",
    "    \"\"\"\n",
    "    创建特征分组。\n",
    "    method: 'external'|'custom'|'auto'\n",
    "      - 'external'：从 external_dir 中读取所有 csv 文件（每个文件视为一组，使用列名映射）\n",
    "      - 'custom'：使用 custom_groups 字典 {group_name: [names_or_substrings,...]}\n",
    "      - 'auto'：使用启发式规则（原始逻辑）\n",
    "    返回: dict {group_name: [indices,...]}\n",
    "    \"\"\"\n",
    "    feature_names = list(feature_names)\n",
    "    groups = {}\n",
    "\n",
    "    if method == 'external':\n",
    "        # 找到目录下的 csv 文件\n",
    "        if os.path.exists(external_dir):\n",
    "            csv_files = sorted(glob.glob(os.path.join(external_dir, '*.csv')))\n",
    "            for fp in csv_files:\n",
    "                gname = os.path.splitext(os.path.basename(fp))[0]\n",
    "                try:\n",
    "                    df = pd.read_csv(fp)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                # 常见情况：文件里有 SampleN 列，去掉\n",
    "                cols = [c for c in df.columns if c.lower() not in ('samplen', 'sample_n', 'sampleid', 'sample_id')]\n",
    "                # 如果文件包含目标列（如 SumTS），自动剔除\n",
    "                cols = [c for c in cols if c not in ('SumTS', 'SumTS'.lower())]\n",
    "                idxs = _find_indices_by_names(feature_names, cols)\n",
    "                if idxs:\n",
    "                    groups[gname] = idxs\n",
    "        # 如果没有读取到任何组，回退到 auto\n",
    "        if not groups:\n",
    "            method = 'auto'\n",
    "\n",
    "    if method == 'custom' and custom_groups:\n",
    "        for gname, name_list in custom_groups.items():\n",
    "            idxs = _find_indices_by_names(feature_names, name_list)\n",
    "            if idxs:\n",
    "                groups[gname] = idxs\n",
    "        # allow fall back to auto if empty\n",
    "        if not groups:\n",
    "            method = 'auto'\n",
    "\n",
    "    if method == 'auto':\n",
    "        # 原始启发式规则的改进版本\n",
    "        ee_elements = {'mn', 'cu', 'fe', 'zn', 'mg', 'k', 'ca', 'mo'}\n",
    "        be_elements = {'v', 'na', 'co'}\n",
    "        hm_elements = {'pb', 'cr', 'cd', 'as', 'sb', 'sn'}\n",
    "        re_elements = {'y', 'la', 'ce', 'pr', 'nd', 'sm', 'eu', 'gd', 'tb', 'dy', 'ho', 'er', 'yb', 'th'}\n",
    "        oe_elements = {'ga', 'rb', 'li', 'sr', 'ba', 'be', 'cs', 'ni', 'sc', 'ge', 'hf', 'nb', 'ti', 'zr'}\n",
    "\n",
    "        tmp = {\n",
    "            '必需元素': [],\n",
    "            '有益元素': [],\n",
    "            '重金属元素': [],\n",
    "            '稀土元素': [],\n",
    "            '其他元素': [],\n",
    "            '土壤元素': [],\n",
    "            '理化性质': [],\n",
    "            '地理信息': [],\n",
    "            '气候环境': [],\n",
    "            '理化+元素': []\n",
    "        }\n",
    "\n",
    "        for i, name in enumerate(feature_names):\n",
    "            nl = name.lower()\n",
    "            # 气候环境关键字（可扩展）\n",
    "            if any(k in nl for k in ['di','gdd0','gdd5','gp','mi','mtco','mtwa','tmax','tmin','precip','ppt','temp']):\n",
    "                tmp['气候环境'].append(i)\n",
    "                continue\n",
    "            # 土壤元素标识\n",
    "            if nl.endswith('_s') or nl.endswith('_S') or nl.endswith('_soil'):\n",
    "                tmp['土壤元素'].append(i)\n",
    "                continue\n",
    "            # 理化性质\n",
    "            if any(k in nl for k in ['ph', 'ec', 'nh4_n', 'no3_n', 'tn', 'om', 'an_x', 'ap_x', 'ak_x', 'soiltypenum', 'cec']):\n",
    "                tmp['理化性质'].append(i)\n",
    "                continue\n",
    "            # 地理信息\n",
    "            if any(k in nl for k in ['longitude', 'latitude', 'alt', 'elevation']):\n",
    "                tmp['地理信息'].append(i)\n",
    "                continue\n",
    "            # 元素分类（尝试解析前缀如 'mn_s'）\n",
    "            token = nl.split('_')[0]\n",
    "            if token in ee_elements:\n",
    "                tmp['必需元素'].append(i)\n",
    "                continue\n",
    "            if token in be_elements:\n",
    "                tmp['有益元素'].append(i)\n",
    "                continue\n",
    "            if token in hm_elements:\n",
    "                tmp['重金属元素'].append(i)\n",
    "                continue\n",
    "            if token in re_elements:\n",
    "                tmp['稀土元素'].append(i)\n",
    "                continue\n",
    "            if token in oe_elements:\n",
    "                tmp['其他元素'].append(i)\n",
    "                continue\n",
    "            # 其它情况可放到理化+元素\n",
    "            if any(ch in nl for ch in ['element', 'conc', 'mg', 'ppm', 'mgkg']):\n",
    "                tmp['理化+元素'].append(i)\n",
    "\n",
    "        # 移除空组\n",
    "        groups = {k: v for k, v in tmp.items() if len(v) > 0}\n",
    "\n",
    "    return groups\n",
    "\n",
    "# --- 示例：三种用法（按需修改） ---\n",
    "# 1) external（优先尝试读取 c:\\\\.../data_R/ElementGroup 中的 CSV）\n",
    "feature_groups = create_feature_groups(feature_names, method='external', external_dir='./data_R/EnvironmentGroup')\n",
    "\n",
    "# 2) custom（若你想手工指定分组规则，取消下面注释并修改）\n",
    "# custom = {\n",
    "#     '必需元素': ['mn','cu','fe','zn','mg','k','ca','mo'],\n",
    "#     '有益元素': ['v','na','co'],\n",
    "#     '重金属': ['pb','cr','cd','as'],\n",
    "# }\n",
    "# feature_groups = create_feature_groups(feature_names, method='custom', custom_groups=custom)\n",
    "\n",
    "# 3) auto（使用启发式规则）\n",
    "# feature_groups = create_feature_groups(feature_names, method='auto')\n",
    "\n",
    "# 打印结果\n",
    "print(f\"创建了 {len(feature_groups)} 个特征组:\")\n",
    "for group_name, indices in feature_groups.items():\n",
    "    print(f\"  - {group_name}: {len(indices)} 个特征\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSDAE Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改初始化范围\n",
    "添加数值稳定性检查\n",
    "        group_loss = tf.clip_by_value(group_loss, 0.0, 1e6)\n",
    "        l1_loss = tf.clip_by_value(l1_loss, 0.0, 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroToOneClip(tf.keras.constraints.Constraint):\n",
    "    \"\"\"权重约束：限制在0-1之间\"\"\"\n",
    "    def __call__(self, w):\n",
    "        return tf.clip_by_value(w, 0, 1)\n",
    "\n",
    "class GroupSelectiveLayer(keras.layers.Layer):\n",
    "    \"\"\"组选择层 - 支持组稀疏正则化的特征选择层\"\"\"\n",
    "    def __init__(self, feature_groups, group_lasso_rate=0.01, l1_rate=0.001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_groups = feature_groups\n",
    "        self.group_lasso_rate = group_lasso_rate\n",
    "        self.l1_rate = l1_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\", \n",
    "            shape=(int(input_shape[-1]),),\n",
    "            \n",
    "            # 修改初始化范围\n",
    "            initializer=initializers.RandomUniform(minval=0.5, maxval=1.0),  # 修改初始化范围\n",
    "            constraint=ZeroToOneClip(),\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        weighted_features = tf.multiply(inputs, self.kernel)\n",
    "        \n",
    "        # 添加组稀疏正则化损失\n",
    "        group_loss = 0.0\n",
    "        for group_indices in self.feature_groups.values():\n",
    "            if len(group_indices) > 0:\n",
    "                group_weights = tf.gather(self.kernel, group_indices)\n",
    "                group_l2_norm = tf.norm(group_weights, ord=2)\n",
    "                group_loss += group_l2_norm\n",
    "        \n",
    "        l1_loss = tf.reduce_sum(tf.abs(self.kernel))\n",
    "        \n",
    "        # 添加数值稳定性检查\n",
    "        group_loss = tf.clip_by_value(group_loss, 0.0, 1e6)\n",
    "        l1_loss = tf.clip_by_value(l1_loss, 0.0, 1e6)\n",
    "        \n",
    "        self.add_loss(self.group_lasso_rate * group_loss)\n",
    "        self.add_loss(self.l1_rate * l1_loss)\n",
    "        \n",
    "        return weighted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里检查一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 创新点：添加组注意力层 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 创新点：添加组注意力层 ---\n",
    "class GroupAttentionLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    组注意力层 (Group Attention Layer)\n",
    "    1. 将输入特征按组分割。\n",
    "    2. 对每组特征进行聚合（例如，通过一个Dense层），生成一个代表该组的向量。\n",
    "    3. 使用多头自注意力机制计算这些组向量之间的关系，并得到注意力权重。\n",
    "    4. 将注意力权重应用回原始的组特征上。\n",
    "    5. 将加权后的组特征重新拼接，作为最终输出。\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_groups, projection_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_groups = feature_groups\n",
    "        self.group_order = list(feature_groups.keys()) # 保证组的顺序固定\n",
    "        self.num_groups = len(self.group_order)\n",
    "        self.projection_dim = projection_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # 用于将每个不同大小的组投影到相同维度的Dense层\n",
    "        self.projection_layers = [Dense(projection_dim, activation='relu', name=f'projection_{name}') for name in self.group_order]\n",
    "        \n",
    "        # 核心：多头注意力层\n",
    "        self.attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, name='group_attention')\n",
    "        self.layer_norm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # 前馈网络\n",
    "        self.ffn = keras.Sequential([\n",
    "            Dense(projection_dim * 2, activation=\"relu\"),\n",
    "            Dense(projection_dim),\n",
    "        ], name='ffn')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch_size, total_features)\n",
    "        \n",
    "        # 1. 按组分割特征\n",
    "        group_tensors = []\n",
    "        for group_name in self.group_order:\n",
    "            indices = self.feature_groups[group_name]\n",
    "            group_tensor = tf.gather(inputs, indices, axis=1)\n",
    "            group_tensors.append(group_tensor)\n",
    "\n",
    "        # 2. 将每个组投影到固定维度\n",
    "        projected_groups = [self.projection_layers[i](group_tensors[i]) for i in range(self.num_groups)]\n",
    "        \n",
    "        # 3. 堆叠成序列，准备输入注意力层\n",
    "        sequence = tf.stack(projected_groups, axis=1)\n",
    "        \n",
    "        # 4. 应用多头自注意力\n",
    "        attention_output, attention_scores = self.attention(\n",
    "            query=sequence, \n",
    "            value=sequence, \n",
    "            key=sequence,\n",
    "            return_attention_scores=True\n",
    "        )\n",
    "        \n",
    "        # 5. 残差连接和层归一化\n",
    "        attn_out_norm = self.layer_norm1(sequence + attention_output)\n",
    "        \n",
    "        # 6. 前馈网络\n",
    "        ffn_output = self.ffn(attn_out_norm)\n",
    "        ffn_out_norm = self.layer_norm2(attn_out_norm + ffn_output)\n",
    "        \n",
    "        # 7. 计算最终的组注意力权重 (用于模型输出)\n",
    "        group_attention_weights = tf.reduce_mean(tf.reduce_sum(attention_scores, axis=2), axis=1)\n",
    "        group_attention_weights = tf.nn.softmax(group_attention_weights, axis=-1)\n",
    "        \n",
    "        # --- 核心修改：使用加权和拼接代替 scatter_update ---\n",
    "        # 8. 将注意力权重应用回原始组特征\n",
    "        weighted_groups = []\n",
    "        for i in range(self.num_groups):\n",
    "            # 获取对应组的原始特征\n",
    "            original_group_tensor = group_tensors[i]\n",
    "            # 获取对应组的注意力权重，并扩展维度以进行广播\n",
    "            weight = group_attention_weights[:, i:i+1] # Shape: (batch_size, 1)\n",
    "            # 加权\n",
    "            weighted_group = original_group_tensor * weight\n",
    "            weighted_groups.append(weighted_group)\n",
    "\n",
    "        # 9. 创建一个全零的列表，用于按原始顺序放置加权后的组\n",
    "        num_features = inputs.shape[1]\n",
    "        # 使用一个字典来按索引存储加权后的组\n",
    "        placeholder = {}\n",
    "        for i, group_name in enumerate(self.group_order):\n",
    "            indices = self.feature_groups[group_name]\n",
    "            weighted_group = weighted_groups[i]\n",
    "            # 将加权后的组按特征维度拆分\n",
    "            split_tensors = tf.split(weighted_group, num_or_size_splits=weighted_group.shape[1], axis=1)\n",
    "            for j, index in enumerate(indices):\n",
    "                placeholder[index] = split_tensors[j]\n",
    "        \n",
    "        # 10. 按原始特征顺序重组张量\n",
    "        final_output_tensors = [placeholder[i] for i in range(num_features)]\n",
    "        final_output = tf.concat(final_output_tensors, axis=1)\n",
    "            \n",
    "        return final_output, group_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 创新点：包含注意力机制的GSDAE模型构建函数 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 创新点：包含注意力机制的GSDAE模型构建函数 ---\n",
    "def build_GSDAE_with_Attention(input_shape, target_dim, feature_groups, \n",
    "                               nbr_hidden_layers=3, hidden_layer_shape=12, \n",
    "                               encodings_nbr=6, group_lasso_rate=0.01, l1_rate=0.001,\n",
    "                               projection_dim=32, num_heads=4):\n",
    "    \"\"\"构建带有组注意力机制的GSDAE模型\"\"\"\n",
    "    \n",
    "    # 输入层\n",
    "    feature_inputs = Input(shape=[input_shape], name='input')\n",
    "    \n",
    "    # 1. 组内特征选择层 (与之前相同)\n",
    "    group_selective_layer = GroupSelectiveLayer(\n",
    "        feature_groups=feature_groups,\n",
    "        group_lasso_rate=group_lasso_rate,\n",
    "        l1_rate=l1_rate,\n",
    "        name='improved_group_selective_layer'\n",
    "    )\n",
    "    selected_features = group_selective_layer(feature_inputs)\n",
    "    \n",
    "    # 2. 组间注意力层 (新添加)\n",
    "    group_attention_layer = GroupAttentionLayer(\n",
    "        feature_groups=feature_groups,\n",
    "        projection_dim=projection_dim,\n",
    "        num_heads=num_heads,\n",
    "        name='group_attention_layer'\n",
    "    )\n",
    "    # 注意力层返回加权后的特征和组的注意力分数\n",
    "    attended_features, group_attention_scores = group_attention_layer(selected_features)\n",
    "    \n",
    "    # --- 新增修改：为注意力分数输出张量赋予明确的名称 ---\n",
    "    # 使用一个无操作的激活层来给张量命名，这是Keras中的一个常用技巧\n",
    "    group_attention_scores = tf.keras.layers.Activation('linear', name='attention_scores')(group_attention_scores)\n",
    "    \n",
    "    # --- 后续结构与原模型基本一致，但输入变为 attended_features ---\n",
    "    \n",
    "    # 编码器\n",
    "    encoder_select = attended_features\n",
    "    for i in range(nbr_hidden_layers):\n",
    "        encoder_select = Dense(hidden_layer_shape, activation='linear', name=f'encoder_hidden_layer_select_{i}')(encoder_select)\n",
    "        encoder_select = keras.layers.LeakyReLU(alpha=0.01)(encoder_select)\n",
    "        encoder_select = keras.layers.BatchNormalization()(encoder_select)\n",
    "    \n",
    "    # 编码层\n",
    "    encoding_select = Dense(encodings_nbr, activation='linear', name='encoding_layer_select')(encoder_select)\n",
    "    encoding_select = keras.layers.LeakyReLU(alpha=0.01)(encoding_select)\n",
    "    \n",
    "    # 预测头\n",
    "    prediction_head = Dense(32, activation='linear', name='pred_hidden')(encoding_select)\n",
    "    prediction_head = keras.layers.LeakyReLU(alpha=0.01)(prediction_head)\n",
    "    prediction_head = keras.layers.BatchNormalization()(prediction_head)\n",
    "    target_prediction = Dense(target_dim, activation='linear', name='target_prediction')(prediction_head)\n",
    "    \n",
    "    # 解码器\n",
    "    decoder_layers = []\n",
    "    for i in range(nbr_hidden_layers):\n",
    "        decoder_layer = Dense(hidden_layer_shape, activation='linear', name=f'decoder_hidden_layer_{i}')\n",
    "        decoder_layers.append(decoder_layer)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 应用解码器\n",
    "    decoder_select = encoding_select\n",
    "    for decoder_layer in decoder_layers:\n",
    "        decoder_select = decoder_layer(decoder_select)\n",
    "        decoder_select = keras.layers.LeakyReLU(alpha=0.01)(decoder_select)\n",
    "        decoder_select = keras.layers.BatchNormalization()(decoder_select)\n",
    "    \n",
    "    # --- 修改：为重构输出张量添加名称 ---\n",
    "    reconstruction_select = Dense(input_shape, activation='linear', name='reconstruction_layer')(decoder_select)\n",
    "    \n",
    "    # 构建模型\n",
    "    # 输出中增加了 group_attention_scores 以便后续分析\n",
    "    gsdae_model = Model(\n",
    "        inputs=feature_inputs, \n",
    "        outputs=[reconstruction_select, target_prediction, group_attention_scores], \n",
    "        name='AttentionGSDAE'\n",
    "    )\n",
    "    \n",
    "    return gsdae_model, group_selective_layer, group_attention_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lasso_rate = 1e-4  # 降低L1正则化率\n",
    "group_lasso_rate = 1e-3  # 降低组稀疏正则化率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型参数和构建新模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数定义完成。\n",
      "\n",
      "带有注意力机制的GSDAE模型结构:\n",
      "Model: \"AttentionGSDAE\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 72)]         0           []                               \n",
      "                                                                                                  \n",
      " improved_group_selective_layer  (None, 72)          72          ['input[0][0]']                  \n",
      "  (GroupSelectiveLayer)                                                                           \n",
      "                                                                                                  \n",
      " group_attention_layer (GroupAt  ((None, 72),        23648       ['improved_group_selective_layer[\n",
      " tentionLayer)                   (None, 4))                      0][0]']                          \n",
      "                                                                                                  \n",
      " encoder_hidden_layer_select_0   (None, 12)          876         ['group_attention_layer[0][0]']  \n",
      " (Dense)                                                                                          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 12)           0           ['encoder_hidden_layer_select_0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 12)          48          ['leaky_re_lu[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " encoder_hidden_layer_select_1   (None, 12)          156         ['batch_normalization[0][0]']    \n",
      " (Dense)                                                                                          \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 12)           0           ['encoder_hidden_layer_select_1[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 12)          48          ['leaky_re_lu_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " encoder_hidden_layer_select_2   (None, 12)          156         ['batch_normalization_1[0][0]']  \n",
      " (Dense)                                                                                          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 12)           0           ['encoder_hidden_layer_select_2[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 12)          48          ['leaky_re_lu_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " encoding_layer_select (Dense)  (None, 6)            78          ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 6)            0           ['encoding_layer_select[0][0]']  \n",
      "                                                                                                  \n",
      " decoder_hidden_layer_0 (Dense)  (None, 12)          84          ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 12)           0           ['decoder_hidden_layer_0[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 12)          48          ['leaky_re_lu_5[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_hidden_layer_1 (Dense)  (None, 12)          156         ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 12)           0           ['decoder_hidden_layer_1[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 12)          48          ['leaky_re_lu_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_hidden_layer_2 (Dense)  (None, 12)          156         ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " pred_hidden (Dense)            (None, 32)           224         ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 12)           0           ['decoder_hidden_layer_2[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 32)           0           ['pred_hidden[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 12)          48          ['leaky_re_lu_7[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32)          128         ['leaky_re_lu_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " reconstruction_layer (Dense)   (None, 72)           936         ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " target_prediction (Dense)      (None, 1)            33          ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " attention_scores (Activation)  (None, 4)            0           ['group_attention_layer[0][1]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,991\n",
      "Trainable params: 26,783\n",
      "Non-trainable params: 208\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 定义模型超参数 ---\n",
    "# 静态重要性层参数\n",
    "group_lasso_rate = 1e-3\n",
    "l1_rate = 1e-4\n",
    "# 注意力层参数\n",
    "projection_dim = 32  # 每个组投影到的维度\n",
    "num_heads = 4        # 注意力头的数量\n",
    "# 网络结构参数\n",
    "hidden_layer_shape = 12\n",
    "nbr_hidden_layers = 3\n",
    "encodings_nbr = 6\n",
    "# 训练参数\n",
    "learning_rate = 1e-3\n",
    "prediction_weight = 0.2 # 预测任务的损失权重\n",
    "batch_size = 32\n",
    "epochs = 150\n",
    "\n",
    "print(\"模型参数定义完成。\")\n",
    "\n",
    "# --- 2. 构建带有注意力机制的GSDAE模型 ---\n",
    "# 注意：我们现在调用新的构建函数\n",
    "AttentionGSDAE, group_selective_layer, group_attention_layer = build_GSDAE_with_Attention(\n",
    "    input_shape=X_train_scal.shape[1],\n",
    "    target_dim=y_train_scal.shape[1],\n",
    "    feature_groups=feature_groups,\n",
    "    nbr_hidden_layers=nbr_hidden_layers,\n",
    "    hidden_layer_shape=hidden_layer_shape,\n",
    "    encodings_nbr=encodings_nbr,\n",
    "    group_lasso_rate=group_lasso_rate,\n",
    "    l1_rate=l1_rate,\n",
    "    projection_dim=projection_dim,\n",
    "    num_heads=num_heads\n",
    ")\n",
    "\n",
    "# --- 3. 打印模型结构以确认 ---\n",
    "print(\"\\n带有注意力机制的GSDAE模型结构:\")\n",
    "AttentionGSDAE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "带有注意力机制的模型编译完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 编译带有注意力机制的模型 ---\n",
    "\n",
    "# 定义一个虚拟损失函数，它永远返回0。用于我们不关心损失的输出。\n",
    "def dummy_loss(y_true, y_pred):\n",
    "    return 0.0\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "\n",
    "# 关键：为模型的三个输出分别指定损失函数和损失权重\n",
    "AttentionGSDAE.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\n",
    "        'reconstruction_layer': 'mean_squared_error',  # 重构输出的损失\n",
    "        'target_prediction': 'mean_squared_error',     # 预测输出的损失\n",
    "        'attention_scores': dummy_loss                 # 注意力分数输出的损失 (虚拟)\n",
    "    },\n",
    "    loss_weights={\n",
    "        'reconstruction_layer': 1.0,                   # 重构损失的权重\n",
    "        'target_prediction': prediction_weight,        # 预测损失的权重\n",
    "        'attention_scores': 0.0                        # 注意力分数损失的权重 (设为0)\n",
    "    },\n",
    "    metrics={\n",
    "        'target_prediction': ['mae', 'mse'] # 我们仍然关心预测任务的指标\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"带有注意力机制的模型编译完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'numpy.ndarray'>\"} values), (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'NoneType'>\", \"<class 'numpy.ndarray'>\"} values)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m开始训练模型...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# --- 修改 fit 的调用方式 ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m history_attention_gsdae = \u001b[43mAttentionGSDAE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     36\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m模型训练完成！\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\anaconda\\envs\\GSDAE\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\anaconda\\envs\\GSDAE\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1082\u001b[39m, in \u001b[36mselect_data_adapter\u001b[39m\u001b[34m(x, y)\u001b[39m\n\u001b[32m   1079\u001b[39m adapter_cls = [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.can_handle(x, y)]\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[32m   1081\u001b[39m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1082\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1083\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to find data adapter that can handle input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   1084\u001b[39m             _type_name(x), _type_name(y)\n\u001b[32m   1085\u001b[39m         )\n\u001b[32m   1086\u001b[39m     )\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) > \u001b[32m1\u001b[39m:\n\u001b[32m   1088\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1089\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mData adapters should be mutually exclusive for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1090\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m to handle \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1091\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(adapter_cls, _type_name(x), _type_name(y))\n\u001b[32m   1092\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'numpy.ndarray'>\"} values), (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'NoneType'>\", \"<class 'numpy.ndarray'>\"} values)"
     ]
    }
   ],
   "source": [
    "# --- 5. 训练带有注意力机制的模型 ---\n",
    "\n",
    "# 关键：为模型的三个输出准备对应的标签字典\n",
    "train_outputs = {\n",
    "    'reconstruction_layer': X_train_scal,\n",
    "    'target_prediction': y_train_scal,\n",
    "    'attention_scores': None  # 注意力分数没有标签，提供None\n",
    "}\n",
    "test_outputs = {\n",
    "    'reconstruction_layer': X_test_scal,\n",
    "    'target_prediction': y_test_scal,\n",
    "    'attention_scores': None\n",
    "}\n",
    "\n",
    "# --- 新增修改：将输入也包装成字典 ---\n",
    "train_inputs = {'input': X_train_scal}\n",
    "test_inputs = {'input': X_test_scal}\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    TerminateOnNaN(),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-6),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "print(\"开始训练模型...\")\n",
    "# --- 修改 fit 的调用方式 ---\n",
    "history_attention_gsdae = AttentionGSDAE.fit(\n",
    "    x=train_inputs, y=train_outputs,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    validation_data=(test_inputs, test_outputs),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n模型训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义analyze_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "def analyze_feature_importance(group_selective_layer, feature_groups, feature_names, \n",
    "                               metric='l2', normalize=True, top_k=None):\n",
    "    \"\"\"两层重要性分析；metric: 'l2'|'l1'|'max'|'mean'；normalize=True时做组大小归一化\"\"\"\n",
    "    weights = group_selective_layer.kernel.numpy()\n",
    "\n",
    "    # 组重要性评估\n",
    "    group_importance = {}\n",
    "    for group_name, indices in feature_groups.items():\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        gw = weights[indices]\n",
    "\n",
    "        # 可选：仅取组内Top-K权重再评估\n",
    "        if top_k is not None and top_k < len(gw):\n",
    "            gw = np.sort(gw)[-top_k:]\n",
    "\n",
    "        if metric == 'l2':\n",
    "            score = np.linalg.norm(gw, ord=2)\n",
    "            if normalize:\n",
    "                score = score / np.sqrt(len(gw))   # RMS：消除组大小影响\n",
    "        elif metric == 'l1':\n",
    "            score = np.sum(np.abs(gw))\n",
    "            if normalize:\n",
    "                score = score / len(gw)           # 平均绝对权重\n",
    "        elif metric == 'max':\n",
    "            score = float(np.max(gw))\n",
    "        else:  # 'mean'\n",
    "            score = float(np.mean(gw))\n",
    "        group_importance[group_name] = float(score)\n",
    "\n",
    "    sorted_groups = sorted(group_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 组内关键特征识别（保持原逻辑）\n",
    "    feature_importance = {}\n",
    "    for group_name, indices in feature_groups.items():\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        gw = weights[indices]\n",
    "        gf = [feature_names[i] for i in indices]\n",
    "        feature_importance[group_name] = sorted(zip(gf, gw), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_groups, feature_importance, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说是要注释掉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training: Full AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSDAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入新的单元格启动自动调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据质量检查\n",
    "print(\"=== 数据质量检查 ===\")\n",
    "print(f\"X_train_scal - 是否包含NaN: {np.isnan(X_train_scal).any()}\")\n",
    "print(f\"X_train_scal - 是否包含inf: {np.isinf(X_train_scal).any()}\")\n",
    "print(f\"X_train_scal - 最大值: {X_train_scal.max():.6f}\")\n",
    "print(f\"X_train_scal - 最小值: {X_train_scal.min():.6f}\")\n",
    "print(f\"X_train_scal - 标准差: {X_train_scal.std():.6f}\")\n",
    "\n",
    "print(f\"y_train_scal - 是否包含NaN: {np.isnan(y_train_scal).any()}\")\n",
    "print(f\"y_train_scal - 是否包含inf: {np.isinf(y_train_scal).any()}\")\n",
    "print(f\"y_train_scal - 最大值: {y_train_scal.max():.6f}\")\n",
    "print(f\"y_train_scal - 最小值: {y_train_scal.min():.6f}\")\n",
    "\n",
    "# 检查是否有异常大的值\n",
    "extreme_threshold = 10\n",
    "extreme_features = np.abs(X_train_scal) > extreme_threshold\n",
    "if extreme_features.any():\n",
    "    print(f\"警告: 发现 {extreme_features.sum()} 个异常大的特征值 (>10)\")\n",
    "\n",
    "print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改进的训练策略 - 解决长期训练NaN问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN问题诊断工具\n",
    "def diagnose_nan_issue(model, X_sample, y_sample):\n",
    "    \"\"\"诊断模型中的NaN问题\"\"\"\n",
    "    print(\"=== NaN问题诊断 ===\")\n",
    "    \n",
    "    # 1. 检查模型权重\n",
    "    print(\"1. 检查模型权重:\")\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel') and layer.kernel is not None:\n",
    "            weights = layer.kernel.numpy()\n",
    "            if np.isnan(weights).any():\n",
    "                print(f\"   ❌ {layer.name}: 发现NaN权重\")\n",
    "            elif np.isinf(weights).any():\n",
    "                print(f\"   ⚠️  {layer.name}: 发现无穷大权重\")\n",
    "            else:\n",
    "                weight_max = np.max(np.abs(weights))\n",
    "                print(f\"   ✅ {layer.name}: 权重正常 (最大绝对值: {weight_max:.6f})\")\n",
    "    \n",
    "    # 2. 检查前向传播\n",
    "    print(\"\\n2. 检查前向传播:\")\n",
    "    try:\n",
    "        outputs = model.predict(X_sample[:5], verbose=0)\n",
    "        if isinstance(outputs, list):\n",
    "            for i, output in enumerate(outputs):\n",
    "                if np.isnan(output).any():\n",
    "                    print(f\"   ❌ 输出{i}: 包含NaN\")\n",
    "                elif np.isinf(output).any():\n",
    "                    print(f\"   ⚠️  输出{i}: 包含无穷大\")\n",
    "                else:\n",
    "                    print(f\"   ✅ 输出{i}: 正常\")\n",
    "        else:\n",
    "            if np.isnan(outputs).any():\n",
    "                print(\"   ❌ 输出: 包含NaN\")\n",
    "            elif np.isinf(outputs).any():\n",
    "                print(\"   ⚠️  输出: 包含无穷大\")\n",
    "            else:\n",
    "                print(\"   ✅ 输出: 正常\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ 前向传播失败: {e}\")\n",
    "    \n",
    "    # 3. 检查损失计算\n",
    "    print(\"\\n3. 检查损失计算:\")\n",
    "    try:\n",
    "        loss = model.evaluate(X_sample[:5], {'reconstruction_layer': X_sample[:5], \n",
    "                                           'target_prediction': y_sample[:5]}, verbose=0)\n",
    "        if np.isnan(loss[0]):\n",
    "            print(f\"   ❌ 总损失: NaN\")\n",
    "        else:\n",
    "            print(f\"   ✅ 总损失: {loss[0]:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ 损失计算失败: {e}\")\n",
    "    \n",
    "    print(\"=\" * 30)\n",
    "\n",
    "# 运行原始模型的诊断（如果已训练）\n",
    "if 'GSDAE' in locals() and 'history_gsdae' in locals():\n",
    "    print(\"诊断原始GSDAE模型:\")\n",
    "    diagnose_nan_issue(GSDAE, X_train_scal, y_train_scal)\n",
    "\n",
    "# 快速训练测试 - 逐步增加轮数观察何时出现NaN\n",
    "def gradual_training_test(model, X_train, y_train, X_val, y_val, max_epochs=50):\n",
    "    \"\"\"逐步增加训练轮数，观察NaN出现的时机\"\"\"\n",
    "    print(\"=== 逐步训练测试 ===\")\n",
    "    \n",
    "    train_outputs = {'reconstruction_layer': X_train, 'target_prediction': y_train}\n",
    "    val_outputs = {'reconstruction_layer': X_val, 'target_prediction': y_val}\n",
    "    \n",
    "    step_sizes = [5, 10, 20, 30, 50]\n",
    "    \n",
    "    for epochs in step_sizes:\n",
    "        if epochs > max_epochs:\n",
    "            break\n",
    "            \n",
    "        print(f\"\\n测试 {epochs} 轮训练:\")\n",
    "        try:\n",
    "            # 重新编译模型确保干净状态\n",
    "            model.compile(\n",
    "                loss={'reconstruction_layer': 'mean_squared_error', 'target_prediction': 'mean_squared_error'},\n",
    "                loss_weights={'reconstruction_layer': 1.0, 'target_prediction': 0.1},\n",
    "                optimizer=optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n",
    "                metrics=['mae']\n",
    "            )\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train, train_outputs,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(X_val, val_outputs),\n",
    "                callbacks=[TerminateOnNaN()],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            final_loss = history.history['loss'][-1]\n",
    "            final_val_loss = history.history['val_loss'][-1]\n",
    "            \n",
    "            if np.isnan(final_loss) or np.isnan(final_val_loss):\n",
    "                print(f\"   ❌ {epochs}轮后出现NaN - 临界点找到！\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"   ✅ {epochs}轮训练正常 (loss: {final_loss:.6f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ {epochs}轮训练失败: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(\"=\" * 30)\n",
    "\n",
    "print(\"可以使用以下函数进行诊断:\")\n",
    "print(\"1. diagnose_nan_issue(model, X_sample, y_sample) - 诊断当前模型状态\")\n",
    "print(\"2. gradual_training_test(model, X_train, y_train, X_val, y_val) - 找到NaN出现的临界点\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 评估 AttentionGSDAE 模型 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 评估 AttentionGSDAE 模型 ---\n",
    "# GSDAE 变量名现在指向 AttentionGSDAE\n",
    "GSDAE = AttentionGSDAE \n",
    "history_gsdae = history_attention_gsdae # 确保 history 对象也指向新的训练历史\n",
    "\n",
    "# 预测会返回一个包含3个元素的列表\n",
    "train_pred_outputs = GSDAE.predict(X_train_scal)\n",
    "test_pred_outputs = GSDAE.predict(X_test_scal)\n",
    "\n",
    "# 解包时忽略第三个输出（注意力分数）\n",
    "train_recon_gsdae, train_target_pred, _ = train_pred_outputs\n",
    "test_recon_gsdae, test_target_pred, _ = test_pred_outputs\n",
    "\n",
    "# 计算误差\n",
    "train_mse_recon = mean_squared_error(X_train_scal, train_recon_gsdae)\n",
    "test_mse_recon = mean_squared_error(X_test_scal, test_recon_gsdae)\n",
    "train_mse_pred = mean_squared_error(y_train_scal, train_target_pred)\n",
    "test_mse_pred = mean_squared_error(y_test_scal, test_target_pred)\n",
    "\n",
    "# 新增：MAE（标准化尺度）\n",
    "train_mae_recon = mean_absolute_error(X_train_scal, train_recon_gsdae)\n",
    "test_mae_recon  = mean_absolute_error(X_test_scal, test_recon_gsdae)\n",
    "train_mae_pred  = mean_absolute_error(y_train_scal, train_target_pred)\n",
    "test_mae_pred   = mean_absolute_error(y_test_scal, test_target_pred)\n",
    "\n",
    "# 反标准化计算R²\n",
    "train_target_pred_orig = scaler_y.inverse_transform(train_target_pred)\n",
    "test_target_pred_orig = scaler_y.inverse_transform(test_target_pred)\n",
    "y_train_orig = scaler_y.inverse_transform(y_train_scal)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test_scal)\n",
    "\n",
    "train_r2 = r2_score(y_train_orig, train_target_pred_orig)\n",
    "test_r2 = r2_score(y_test_orig, test_target_pred_orig)\n",
    "\n",
    "# 新增：目标MAE（原单位，便于解读）\n",
    "train_mae_pred_orig = mean_absolute_error(y_train_orig, train_target_pred_orig)\n",
    "test_mae_pred_orig  = mean_absolute_error(y_test_orig,  test_target_pred_orig)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Attention GSDAE 模型评估结果:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"重构任务 - 训练MSE: {train_mse_recon:.6f}\")\n",
    "print(f\"重构任务 - 测试MSE: {test_mse_recon:.6f}\")\n",
    "print(f\"重构任务 - 训练MAE: {train_mae_recon:.6f}\")\n",
    "print(f\"重构任务 - 测试MAE: {test_mae_recon:.6f}\")\n",
    "print(f\"预测任务 - 训练MSE: {train_mse_pred:.6f}\")\n",
    "print(f\"预测任务 - 测试MSE: {test_mse_pred:.6f}\")\n",
    "print(f\"预测任务 - 训练MAE(标准化): {train_mae_pred:.6f}\")\n",
    "print(f\"预测任务 - 测试MAE(标准化): {test_mae_pred:.6f}\")\n",
    "print(f\"目标预测 - 训练MAE(原单位): {train_mae_pred_orig:.6f}\")\n",
    "print(f\"目标预测 - 测试MAE(原单位): {test_mae_pred_orig:.6f}\")\n",
    "print(f\"目标预测 - 训练R²: {train_r2:.4f}\")\n",
    "print(f\"目标预测 - 测试R²: {test_r2:.4f}\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 新增：动态组注意力分析 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 获取测试集的注意力分数\n",
    "# test_pred_outputs 是一个列表: [reconstruction, prediction, attention_scores]\n",
    "_, _, test_attention_scores = test_pred_outputs\n",
    "\n",
    "# 2. 将分数和组名对应起来，创建DataFrame\n",
    "group_names = list(feature_groups.keys())\n",
    "attention_df = pd.DataFrame(test_attention_scores, columns=group_names, index=X_test.index)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"动态组注意力分析:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"测试集样本的注意力分数 (前5个样本):\")\n",
    "print(attention_df.head())\n",
    "\n",
    "# 3. 可视化注意力分数\n",
    "\n",
    "# a. 绘制注意力分数的整体分布热力图\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(attention_df, cmap='viridis', xticklabels=True, yticklabels=False)\n",
    "plt.title('测试集各样本的组注意力权重热力图', fontsize=16)\n",
    "plt.xlabel('特征组', fontsize=12)\n",
    "plt.ylabel('样本 (未显示标签)', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# b. 绘制平均注意力重要性\n",
    "mean_attention = attention_df.mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "mean_attention.plot(kind='bar', color='teal')\n",
    "plt.title('所有测试样本的平均组注意力', fontsize=16)\n",
    "plt.ylabel('平均注意力权重', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# c. 分析高产/低产样本的注意力差异 (示例)\n",
    "# 合并预测结果和注意力分数\n",
    "y_test_pred_df = pd.DataFrame(test_target_pred_orig, index=X_test.index, columns=['predicted_value'])\n",
    "attention_with_pred = pd.concat([attention_df, y_test_pred_df], axis=1)\n",
    "\n",
    "# 根据预测值定义高产和低产组\n",
    "quantile_threshold = attention_with_pred['predicted_value'].quantile(0.75)\n",
    "high_yield_samples = attention_with_pred[attention_with_pred['predicted_value'] >= quantile_threshold]\n",
    "low_yield_samples = attention_with_pred[attention_with_pred['predicted_value'] < quantile_threshold]\n",
    "\n",
    "# 计算并比较两组的平均注意力\n",
    "mean_attn_high = high_yield_samples[group_names].mean().rename('高产组')\n",
    "mean_attn_low = low_yield_samples[group_names].mean().rename('低产组')\n",
    "comparison_df = pd.concat([mean_attn_high, mean_attn_low], axis=1)\n",
    "\n",
    "print(\"\\n高产 vs 低产样本的平均注意力对比:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# 绘制对比图\n",
    "comparison_df.plot(kind='bar', figsize=(12, 7))\n",
    "plt.title('高产 vs 低产样本的平均组注意力对比', fontsize=16)\n",
    "plt.ylabel('平均注意力权重', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='样本分组')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从训练好的GSDAE模型中按名称获取组选择层的实例\n",
    "group_selective_layer_instance = GSDAE.get_layer('improved_group_selective_layer')\n",
    "\n",
    "# 使用RMS（L2范数并归一化）作为组评分标准进行分析\n",
    "sorted_groups, feature_importance, weights = analyze_feature_importance(\n",
    "    group_selective_layer_instance, \n",
    "    feature_groups, # 注意：这里使用的是最后一次实验的 feature_groups\n",
    "    feature_names,\n",
    "    metric='l2', \n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"最终模型 - 特征重要性分析:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n特征组重要性排名 (RMS):\")\n",
    "for i, (group_name, importance) in enumerate(sorted_groups):\n",
    "    print(f\"{i+1:2d}. {group_name:12s}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\n各组内关键特征 (前3个):\")\n",
    "for group_name, features in feature_importance.items():\n",
    "    print(f\"\\n{group_name}:\")\n",
    "    for i, (feat_name, weight) in enumerate(features[:3]):\n",
    "        print(f\"  {i+1}. {feat_name}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化结果\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# 组重要性\n",
    "top_n_groups = min(8, len(sorted_groups))\n",
    "groups = [item[0] for item in sorted_groups[:top_n_groups]]\n",
    "importance = [item[1] for item in sorted_groups[:top_n_groups]]\n",
    "\n",
    "axes[0, 0].barh(groups, importance, color='skyblue')\n",
    "axes[0, 0].set_xlabel('组重要性 (L2范数)')\n",
    "axes[0, 0].set_title('特征组重要性排名')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 最重要组的关键特征\n",
    "if sorted_groups:\n",
    "    top_group = sorted_groups[0][0]\n",
    "    top_features = feature_importance[top_group][:5]\n",
    "    \n",
    "    feature_names_short = [item[0][:15] + '...' if len(item[0]) > 15 else item[0] for item in top_features]\n",
    "    feature_weights = [item[1] for item in top_features]\n",
    "    \n",
    "    axes[0, 1].barh(feature_names_short, feature_weights, color='lightcoral')\n",
    "    axes[0, 1].set_xlabel('特征权重')\n",
    "    axes[0, 1].set_title(f'\"{top_group}\"组内关键特征')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 训练损失\n",
    "axes[1, 0].plot(history_gsdae.history['loss'], label='训练损失')\n",
    "axes[1, 0].plot(history_gsdae.history['val_loss'], label='验证损失')\n",
    "axes[1, 0].set_title('训练损失')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 预测准确性\n",
    "axes[1, 1].scatter(y_test_orig, test_target_pred_orig, alpha=0.6)\n",
    "axes[1, 1].plot([y_test_orig.min(), y_test_orig.max()], [y_test_orig.min(), y_test_orig.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_xlabel('实际值')\n",
    "axes[1, 1].set_ylabel('预测值')\n",
    "axes[1, 1].set_title(f'预测 vs 实际 (R² = {test_r2:.3f})')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\\n\n",
    "\\n\n",
    "### GSDAE模型总结\\n\n",
    "\\n\n",
    "本notebook实现了基于方案文档的GSDAE模型，主要改进包括：\\n\n",
    "\\n\n",
    "1. **组稀疏正则化**: 针对生态因子的天然分组结构\\n\n",
    "2. **半监督学习**: 引入丹参酮含量作为监督信号\\n\n",
    "3. **预测头**: 专门的预测分支用于药效成分含量预测\\n\n",
    "4. **复合损失函数**: 重构误差 + 预测误差 + 组稀疏正则化\\n\n",
    "5. **两层重要性分析**: 组重要性评估 + 组内关键特征识别\\n\n",
    "\\n\n",
    "### 模型优势\\n\n",
    "\\n\n",
    "- **生物学意义**: 特征分组符合生态因子的天然结构\\n\n",
    "- **目标导向**: 半监督学习机制与药效预测任务对齐\\n\n",
    "- **可解释性**: 提供宏观到微观的多层次分析结果\\n\n",
    "- **实用性**: 同时完成特征选择、降维和预测任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 绘制图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化：各特征组 Top-10 特征权重分布面板（类似图1）\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "# 设置中文字体显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def _get_layer_kernel(layer):\n",
    "    if hasattr(layer, \"kernel\") and layer.kernel is not None:\n",
    "        w = layer.kernel.numpy()\n",
    "        return np.asarray(w, dtype=float)\n",
    "    raise ValueError(\"layer 不含 kernel 权重，请确认传入的是 GroupSelectiveLayer/ImprovedGroupSelectiveLayer 的实例。\")\n",
    "\n",
    "def compute_group_feature_weights(layer, feature_groups, feature_names):\n",
    "    \"\"\"返回 {组名: [(特征名, 权重), ...]}\"\"\"\n",
    "    weights = _get_layer_kernel(layer)\n",
    "    result = {}\n",
    "    for g, idx in feature_groups.items():\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        pairs = [(feature_names[i], float(weights[i])) for i in idx]\n",
    "        # 只保留非负权重（该层已约束到[0,1]，此处为稳健性）\n",
    "        pairs = [(n, max(0.0, w)) for n, w in pairs]\n",
    "        result[g] = pairs\n",
    "    return result\n",
    "\n",
    "def plot_group_weight_panels(group_feature_weights, top_k=10, group_order=None, ncols=4):\n",
    "    \"\"\"绘制各组Top-K特征权重面板\"\"\"\n",
    "    # 组显示顺序\n",
    "    keys = list(group_feature_weights.keys())\n",
    "    if group_order:\n",
    "        ordered = [g for g in group_order if g in group_feature_weights] + [g for g in keys if g not in group_order]\n",
    "    else:\n",
    "        ordered = keys\n",
    "\n",
    "    ordered_map = OrderedDict((g, group_feature_weights[g]) for g in ordered)\n",
    "    ng = len(ordered_map)\n",
    "    ncols = min(ncols, max(1, ng))\n",
    "    nrows = int(np.ceil(ng / ncols))\n",
    "\n",
    "    # 统一x轴上限，便于对比\n",
    "    global_max = 0.0\n",
    "    for feats in ordered_map.values():\n",
    "        if feats:\n",
    "            global_max = max(global_max, max(w for _, w in feats))\n",
    "    global_max = max(global_max, 1e-6) * 1.05\n",
    "\n",
    "    # 颜色：每组一个主色\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=max(10, ng))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4.6*ncols+0.6, 3.2*nrows+1), squeeze=False, sharex=True)\n",
    "    ax_list = axes.flatten()\n",
    "\n",
    "    # 先关掉所有坐标轴，避免空格子显示框线\n",
    "    for ax in ax_list:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for i, (g, feats) in enumerate(ordered_map.items()):\n",
    "        ax = ax_list[i]\n",
    "        ax.axis(\"on\")\n",
    "        feats_sorted = sorted(feats, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        # 反向画水平条，便于从上到下显示从小到大\n",
    "        names = [n if len(n) <= 18 else n[:18] + \"…\" for n, _ in feats_sorted][::-1]\n",
    "        vals = [v for _, v in feats_sorted][::-1]\n",
    "\n",
    "        ax.barh(names, vals, color=palette[i % len(palette)], alpha=0.9)\n",
    "        ax.set_title(g, fontsize=12, pad=6)\n",
    "        ax.set_xlim(0, global_max)\n",
    "        ax.grid(True, axis='x', alpha=0.3)\n",
    "        ax.tick_params(axis='y', labelsize=9)\n",
    "\n",
    "        # 数值标注\n",
    "        for y, v in enumerate(vals):\n",
    "            ax.text(v, y, f\" {v:.3f}\", va='center', ha='left', fontsize=8)\n",
    "\n",
    "    # 总标题与公共x轴标签\n",
    "    fig.suptitle(\"SDAE/GSDAE 各特征组权重分布（每组Top-10）\", fontsize=14)\n",
    "    fig.text(0.5, 0.01, \"权重值\", ha='center', fontsize=11)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# 使用示例（训练完成后运行）\n",
    "# 从训练好的GSDAE模型中按名称获取组选择层的实例\n",
    "try:\n",
    "    layer_inst = GSDAE.get_layer('improved_group_selective_layer')\n",
    "    if layer_inst is None:\n",
    "        raise RuntimeError(\"在GSDAE模型中未找到名为 'improved_group_selective_layer' 的层。\")\n",
    "except NameError:\n",
    "    raise NameError(\"模型 'GSDAE' 未定义。请确保前面的模型训练单元格已成功运行。\")\n",
    "\n",
    "\n",
    "group_feature_weights = compute_group_feature_weights(layer_inst, feature_groups, feature_names)\n",
    "\n",
    "# 推荐的组显示顺序（若某组不存在会自动跳过）\n",
    "preferred_order = [\"土壤元素\", \"作物元素\", \"土壤养分\", \"地理信息\", \"气候环境\",\n",
    "                   \"省份\", \"城市\", \"地貌\", \"土壤类型\", \"栽培类型\", \"气候类型\"]\n",
    "\n",
    "plot_group_weight_panels(group_feature_weights, top_k=10, group_order=preferred_order, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文字体与“0权重可见”的权重面板图\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "from matplotlib import font_manager as fm\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "def set_cn_font():\n",
    "    \"\"\"自动注册并启用中文字体\"\"\"\n",
    "    candidates = [\n",
    "        r\"C:\\Windows\\Fonts\\msyh.ttc\",   # 微软雅黑\n",
    "        r\"C:\\Windows\\Fonts\\msyh.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\simhei.ttf\", # 黑体\n",
    "        r\"C:\\Windows\\Fonts\\simsun.ttc\"  # 宋体\n",
    "    ]\n",
    "    chosen = None\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                fm.fontManager.addfont(p)\n",
    "                chosen = FontProperties(fname=p).get_name()\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if chosen is None:\n",
    "        # 回退到常用名称\n",
    "        chosen = \"Microsoft YaHei\"\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = [chosen, 'SimHei', 'SimSun', 'DejaVu Sans']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    print(f\"已设置中文字体: {chosen}\")\n",
    "\n",
    "set_cn_font()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def _get_layer_kernel(layer):\n",
    "    if hasattr(layer, \"kernel\") and layer.kernel is not None:\n",
    "        w = layer.kernel.numpy()\n",
    "        return np.asarray(w, dtype=float)\n",
    "    raise ValueError(\"layer 不含 kernel 权重，请确认传入的是 GroupSelectiveLayer/ImprovedGroupSelectiveLayer 的实例。\")\n",
    "\n",
    "def compute_group_feature_weights(layer, feature_groups, feature_names):\n",
    "    \"\"\"返回 {组名: [(特征名, 权重), ...]}（包含0值）\"\"\"\n",
    "    weights = _get_layer_kernel(layer)\n",
    "    result = {}\n",
    "    for g, idx in feature_groups.items():\n",
    "        \n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        pairs = [(feature_names[i], float(max(0.0, weights[i]))) for i in idx]  # 负值截断为0，保留0\n",
    "        result[g] = pairs\n",
    "    return result\n",
    "\n",
    "def plot_group_weight_panels(group_feature_weights, top_k=None, group_order=None, ncols=4, zero_min_ratio=0.02):\n",
    "    \"\"\"\n",
    "    绘制各组Top-K特征权重面板\n",
    "    - top_k=None 显示组内全部特征\n",
    "    - zero_min_ratio: 0值条形显示为 (global_max * ratio) 的最小宽度\n",
    "    \"\"\"\n",
    "    # 组显示顺序\n",
    "    keys = list(group_feature_weights.keys())\n",
    "    if group_order:\n",
    "        ordered = [g for g in group_order if g in group_feature_weights] + [g for g in keys if g not in group_order]\n",
    "    else:\n",
    "        ordered = keys\n",
    "\n",
    "    ordered_map = OrderedDict((g, group_feature_weights[g]) for g in ordered)\n",
    "    ng = len(ordered_map)\n",
    "    ncols = min(ncols, max(1, ng))\n",
    "    nrows = int(np.ceil(ng / ncols))\n",
    "\n",
    "    # 统一x轴上限，便于对比（按真实权重计算）\n",
    "    all_vals = [w for feats in ordered_map.values() for _, w in feats] or [0.0]\n",
    "    global_max = float(max(all_vals))\n",
    "    if global_max <= 0:\n",
    "        global_max = 1.0  # 全0时的可视上限\n",
    "    xlim = global_max * 1.05\n",
    "    min_bar = max(global_max * zero_min_ratio, 1e-6)  # 0值条的最小显示宽度\n",
    "\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=max(10, ng))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4.8*ncols+0.6, 3.2*nrows+1), squeeze=False, sharex=True)\n",
    "    ax_list = axes.flatten()\n",
    "\n",
    "    # 先关掉所有坐标轴，避免空格子显示框线\n",
    "    for ax in ax_list:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for i, (g, feats) in enumerate(ordered_map.items()):\n",
    "        ax = ax_list[i]\n",
    "        ax.axis(\"on\")\n",
    "        feats_sorted = sorted(feats, key=lambda x: x[1], reverse=True)\n",
    "        if top_k is not None:\n",
    "            feats_sorted = feats_sorted[:top_k]\n",
    "\n",
    "        names = [n if len(n) <= 18 else n[:18] + \"…\" for n, _ in feats_sorted][::-1]\n",
    "        vals_true = [v for _, v in feats_sorted][::-1]\n",
    "        # 0值条形使用最小宽度渲染，但保留真实数值标注\n",
    "        vals_plot = [v if v > 0 else min_bar for v in vals_true]\n",
    "\n",
    "        ax.barh(names, vals_plot, color=palette[i % len(palette)], alpha=0.9)\n",
    "        ax.set_title(g, fontsize=12, pad=6)\n",
    "        ax.set_xlim(0, xlim)\n",
    "        ax.grid(True, axis='x', alpha=0.3)\n",
    "        ax.tick_params(axis='y', labelsize=9)\n",
    "\n",
    "        # 数值标注（使用真实值；位置用绘制宽度以保证可见）\n",
    "        for y, (vp, vt) in enumerate(zip(vals_plot, vals_true)):\n",
    "            ax.text(vp, y, f\" {vt:.3f}\", va='center', ha='left', fontsize=8)\n",
    "\n",
    "    fig.suptitle(\"SDAE/GSDAE 各特征组权重分布（含0值；每组Top-K）\", fontsize=14)\n",
    "    fig.text(0.5, 0.01, \"权重值\", ha='center', fontsize=11)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "# 从训练好的GSDAE模型中按名称获取组选择层的实例\n",
    "try:\n",
    "    layer_inst = GSDAE.get_layer('improved_group_selective_layer')\n",
    "    if layer_inst is None:\n",
    "        raise RuntimeError(\"在GSDAE模型中未找到名为 'improved_group_selective_layer' 的层。\")\n",
    "except NameError:\n",
    "    raise NameError(\"模型 'GSDAE' 未定义。请确保前面的模型训练单元格已成功运行。\")\n",
    "\n",
    "\n",
    "group_feature_weights = compute_group_feature_weights(layer_inst, feature_groups, feature_names)\n",
    "# 推荐顺序；top_k=None 显示组内全部特征（包含0）\n",
    "preferred_order = [\"土壤元素\", \"作物元素\", \"土壤养分\", \"地理信息\", \"气候环境\",\n",
    "                   \"省份\", \"城市\", \"地貌\", \"土壤类型\", \"栽培类型\", \"气候类型\"]\n",
    "\n",
    "plot_group_weight_panels(group_feature_weights, top_k=None, group_order=preferred_order, ncols=4, zero_min_ratio=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. 创建结果输出目录 ---\n",
    "output_dir = 'results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"结果将保存在 '{output_dir}' 文件夹中。\\n\")\n",
    "\n",
    "# --- 2. 准备文本摘要内容 ---\n",
    "summary_content = []\n",
    "summary_content.append(\"=\" * 50)\n",
    "summary_content.append(f\"GSDAE 模型分析报告 ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "summary_content.append(\"=\" * 50)\n",
    "\n",
    "# a. 最佳超参数\n",
    "if 'best_hps' in locals():\n",
    "    summary_content.append(\"\\n### 1. 最佳超参数 ###\")\n",
    "    summary_content.append(f\" - learning_rate: {best_hps.get('learning_rate')}\")\n",
    "    summary_content.append(f\" - group_lasso_rate: {best_hps.get('group_lasso_rate')}\")\n",
    "    summary_content.append(f\" - l1_rate: {best_hps.get('l1_rate')}\")\n",
    "    summary_content.append(f\" - prediction_weight: {best_hps.get('prediction_weight')}\")\n",
    "\n",
    "# b. 模型评估指标\n",
    "summary_content.append(\"\\n### 2. 模型评估指标 ###\")\n",
    "summary_content.append(f\" - 目标预测 - 测试 R²: {test_r2:.4f}\")\n",
    "summary_content.append(f\" - 目标预测 - 测试 MAE (原单位): {test_mae_pred_orig:.6f}\")\n",
    "summary_content.append(f\" - 目标预测 - 训练 R²: {train_r2:.4f}\")\n",
    "summary_content.append(f\" - 目标预测 - 训练 MAE (原单位): {train_mae_pred_orig:.6f}\")\n",
    "summary_content.append(\"-\" * 20)\n",
    "summary_content.append(f\" - 重构任务 - 测试 MSE: {test_mse_recon:.6f}\")\n",
    "summary_content.append(f\" - 预测任务 - 测试 MSE: {test_mse_pred:.6f}\")\n",
    "\n",
    "# c. 特征重要性\n",
    "if 'sorted_groups' in locals() and 'feature_importance' in locals():\n",
    "    summary_content.append(\"\\n### 3. 特征组重要性排名 (RMS) ###\")\n",
    "    for i, (group_name, importance) in enumerate(sorted_groups):\n",
    "        summary_content.append(f\"{i+1:2d}. {group_name:12s}: {importance:.4f}\")\n",
    "\n",
    "    summary_content.append(\"\\n### 4. 各组内关键特征 (Top 3) ###\")\n",
    "    for group_name, features in feature_importance.items():\n",
    "        summary_content.append(f\"\\n--- {group_name} ---\")\n",
    "        for i, (feat_name, weight) in enumerate(features[:3]):\n",
    "            summary_content.append(f\"  {i+1}. {feat_name}: {weight:.4f}\")\n",
    "\n",
    "# --- 3. 打印并保存文本摘要 ---\n",
    "summary_text = \"\\n\".join(summary_content)\n",
    "print(summary_text)\n",
    "\n",
    "summary_filepath = os.path.join(output_dir, 'model_summary.txt')\n",
    "with open(summary_filepath, 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_text)\n",
    "print(f\"\\n✅ 文本摘要已保存至: {summary_filepath}\")\n",
    "\n",
    "\n",
    "# --- 4. 重新生成并保存所有图表 ---\n",
    "\n",
    "# a. 保存综合评估图\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "# 组重要性\n",
    "top_n_groups = min(8, len(sorted_groups))\n",
    "groups = [item[0] for item in sorted_groups[:top_n_groups]]\n",
    "importance = [item[1] for item in sorted_groups[:top_n_groups]]\n",
    "axes[0, 0].barh(groups, importance, color='skyblue')\n",
    "axes[0, 0].set_xlabel('组重要性 (RMS)')\n",
    "axes[0, 0].set_title('特征组重要性排名')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "# 最重要组的关键特征\n",
    "if sorted_groups:\n",
    "    top_group = sorted_groups[0][0]\n",
    "    top_features = feature_importance[top_group][:5]\n",
    "    feature_names_short = [item[0][:15] + '...' if len(item[0]) > 15 else item[0] for item in top_features]\n",
    "    feature_weights = [item[1] for item in top_features]\n",
    "    axes[0, 1].barh(feature_names_short, feature_weights, color='lightcoral')\n",
    "    axes[0, 1].set_xlabel('特征权重')\n",
    "    axes[0, 1].set_title(f'\"{top_group}\"组内关键特征')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "# 训练损失\n",
    "axes[1, 0].plot(history_gsdae.history['loss'], label='训练损失')\n",
    "axes[1, 0].plot(history_gsdae.history['val_loss'], label='验证损失')\n",
    "axes[1, 0].set_title('训练损失')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "# 预测准确性\n",
    "axes[1, 1].scatter(y_test_orig, test_target_pred_orig, alpha=0.6)\n",
    "axes[1, 1].plot([y_test_orig.min(), y_test_orig.max()], [y_test_orig.min(), y_test_orig.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_xlabel('实际值')\n",
    "axes[1, 1].set_ylabel('预测值')\n",
    "axes[1, 1].set_title(f'预测 vs 实际 (R² = {test_r2:.3f})')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "eval_fig_path = os.path.join(output_dir, 'evaluation_summary.png')\n",
    "plt.savefig(eval_fig_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"✅ 综合评估图已保存至: {eval_fig_path}\")\n",
    "\n",
    "# b. 保存 Top-10 特征权重图\n",
    "plot_group_weight_panels(group_feature_weights, top_k=10, group_order=preferred_order, ncols=4)\n",
    "top10_fig_path = os.path.join(output_dir, 'feature_weights_top10.png')\n",
    "plt.savefig(top10_fig_path, dpi=300)\n",
    "print(f\"✅ Top-10 特征权重图已保存至: {top10_fig_path}\")\n",
    "\n",
    "\n",
    "# c. 保存全特征权重图\n",
    "plot_group_weight_panels(group_feature_weights, top_k=None, group_order=preferred_order, ncols=4, zero_min_ratio=0.02)\n",
    "all_weights_fig_path = os.path.join(output_dir, 'feature_weights_all.png')\n",
    "plt.savefig(all_weights_fig_path, dpi=300)\n",
    "print(f\"✅ 全特征权重图已保存至: {all_weights_fig_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GSDAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
