#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GSDAE (Group Selective Deep AutoEncoder) for Danshen Analysis
åŸºäºæ–¹æ¡ˆæ–‡æ¡£çš„æ”¹è¿›ç‰ˆæœ¬ï¼ŒåŒ…å«ï¼š
1. ç»„ç¨€ç–æ­£åˆ™åŒ– (Group Lasso)
2. åŠç›‘ç£å­¦ä¹ æœºåˆ¶
3. é¢„æµ‹å¤´ (Prediction Head)
4. å¤åˆæŸå¤±å‡½æ•°
5. ä¸¤å±‚é‡è¦æ€§åˆ†æ
"""

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

from sklearn.preprocessing import StandardScaler
import random
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import mean_squared_error, r2_score
from tensorflow.keras import regularizers

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input, Dropout
from tensorflow.keras import optimizers, initializers

class ZeroToOneClip(tf.keras.constraints.Constraint):
    """æƒé‡çº¦æŸï¼šé™åˆ¶åœ¨0-1ä¹‹é—´"""
    def __call__(self, w):
        return tf.clip_by_value(w, 0, 1)

class GroupSelectiveLayer(keras.layers.Layer):
    """ç»„é€‰æ‹©å±‚ - æ”¯æŒç»„ç¨€ç–æ­£åˆ™åŒ–çš„ç‰¹å¾é€‰æ‹©å±‚"""
    def __init__(self, feature_groups, group_lasso_rate=0.01, l1_rate=0.001, **kwargs):
        super().__init__(**kwargs)
        self.feature_groups = feature_groups
        self.group_lasso_rate = group_lasso_rate
        self.l1_rate = l1_rate
        
    def build(self, input_shape):
        self.kernel = self.add_weight(
            "kernel", 
            shape=(int(input_shape[-1]),),
            initializer=initializers.RandomUniform(minval=0.999, maxval=1.0),
            constraint=ZeroToOneClip(),
            trainable=True
        )
        
    def call(self, inputs):
        weighted_features = tf.multiply(inputs, self.kernel)
        
        # æ·»åŠ ç»„ç¨€ç–æ­£åˆ™åŒ–æŸå¤±
        group_loss = 0.0
        for group_indices in self.feature_groups.values():
            if len(group_indices) > 0:
                group_weights = tf.gather(self.kernel, group_indices)
                group_l2_norm = tf.norm(group_weights, ord=2)
                group_loss += group_l2_norm
        
        l1_loss = tf.reduce_sum(tf.abs(self.kernel))
        
        self.add_loss(self.group_lasso_rate * group_loss)
        self.add_loss(self.l1_rate * l1_loss)
        
        return weighted_features

def create_feature_groups(feature_names):
    """æ ¹æ®ç‰¹å¾åç§°åˆ›å»ºç‰¹å¾åˆ†ç»„"""
    groups = {
        'åœŸå£¤å…ƒç´ ': [], 'ä½œç‰©å…ƒç´ ': [], 'åœŸå£¤å…»åˆ†': [], 'åœ°ç†ä¿¡æ¯': [], 'æ°”å€™ç¯å¢ƒ': [],
        'çœä»½': [], 'åŸå¸‚': [], 'åœ°è²Œ': [], 'åœŸå£¤ç±»å‹': [], 'æ ½åŸ¹ç±»å‹': [], 'æ°”å€™ç±»å‹': []
    }
    
    for i, name in enumerate(feature_names):
        name_lower = name.lower()
        
        if name.endswith('_S'):
            groups['åœŸå£¤å…ƒç´ '].append(i)
        elif name.endswith('_P'):
            groups['ä½œç‰©å…ƒç´ '].append(i)
        elif any(nutrient in name_lower for nutrient in ['ph', 'om', 'tn', 'tp', 'tk', 'an', 'ap', 'ak']):
            groups['åœŸå£¤å…»åˆ†'].append(i)
        elif any(geo in name_lower for geo in ['lat', 'lon', 'alt', 'elevation']):
            groups['åœ°ç†ä¿¡æ¯'].append(i)
        elif any(climate in name_lower for climate in ['temp', 'prec', 'humi', 'wind', 'sun']):
            groups['æ°”å€™ç¯å¢ƒ'].append(i)
        elif 'province' in name_lower:
            groups['çœä»½'].append(i)
        elif 'city' in name_lower:
            groups['åŸå¸‚'].append(i)
        elif 'landscape' in name_lower:
            groups['åœ°è²Œ'].append(i)
        elif 'soiltype' in name_lower or 'soilclass' in name_lower:
            groups['åœŸå£¤ç±»å‹'].append(i)
        elif 'cultivation' in name_lower:
            groups['æ ½åŸ¹ç±»å‹'].append(i)
        elif 'climate' in name_lower and 'type' in name_lower:
            groups['æ°”å€™ç±»å‹'].append(i)
    
    # ç§»é™¤ç©ºç»„
    groups = {k: v for k, v in groups.items() if len(v) > 0}
    return groups

def build_GSDAE(input_shape, target_dim, feature_groups, nbr_hidden_layers=3, 
                hidden_layer_shape=12, encodings_nbr=6, activation="relu", 
                group_lasso_rate=0.01, l1_rate=0.001):
    """æ„å»ºGSDAEæ¨¡å‹"""
    
    # è¾“å…¥å±‚
    feature_inputs = Input(shape=[input_shape], name='input')
    
    # ç»„é€‰æ‹©å±‚
    group_selective_layer = GroupSelectiveLayer(
        feature_groups=feature_groups,
        group_lasso_rate=group_lasso_rate,
        l1_rate=l1_rate,
        name='group_selective_layer'
    )
    selected_features = group_selective_layer(feature_inputs)
    
    # ç¼–ç å™¨ - åŸå§‹ç‰¹å¾è·¯å¾„
    encoder_full = feature_inputs
    for i in range(nbr_hidden_layers):
        encoder_full = Dense(hidden_layer_shape, activation=activation, 
                           name=f'encoder_hidden_layer_full_{i}')(encoder_full)
    
    # ç¼–ç å™¨ - é€‰æ‹©ç‰¹å¾è·¯å¾„  
    encoder_select = selected_features
    for i in range(nbr_hidden_layers):
        encoder_select = Dense(hidden_layer_shape, activation=activation, 
                             name=f'encoder_hidden_layer_select_{i}')(encoder_select)
    
    # ç¼–ç å±‚
    encoding_full = Dense(encodings_nbr, activation=activation, name='encoding_layer_full')(encoder_full)
    encoding_select = Dense(encodings_nbr, activation=activation, name='encoding_layer_select')(encoder_select)
    
    # é¢„æµ‹å¤´ - ç”¨äºåŠç›‘ç£å­¦ä¹ 
    prediction_head = Dense(32, activation='relu', name='pred_hidden')(encoding_select)
    target_prediction = Dense(target_dim, activation='linear', name='target_prediction')(prediction_head)
    
    # è§£ç å™¨ - å…±äº«æƒé‡
    decoder_layers = []
    for i in range(nbr_hidden_layers):
        decoder_layer = Dense(hidden_layer_shape, activation=activation, name=f'decoder_hidden_layer_{i}')
        decoder_layers.append(decoder_layer)
    
    reconstruction_layer = Dense(input_shape, activation='linear', name='reconstruction_layer')
    
    # åº”ç”¨è§£ç å™¨
    decoder_full = encoding_full
    decoder_select = encoding_select
    
    for decoder_layer in decoder_layers:
        decoder_full = decoder_layer(decoder_full)
        decoder_select = decoder_layer(decoder_select)
    
    # é‡æ„è¾“å‡º
    reconstruction_full = reconstruction_layer(decoder_full)
    reconstruction_select = reconstruction_layer(decoder_select)
    
    # æ„å»ºæ¨¡å‹
    gsdae_model = Model(inputs=feature_inputs, outputs=[reconstruction_select, target_prediction], name='GSDAE')
    full_autoencoder = Model(inputs=feature_inputs, outputs=reconstruction_full, name='FullAutoEncoder')
    feature_selector = Model(inputs=feature_inputs, outputs=selected_features, name='FeatureSelector')
    full_encoder = Model(inputs=feature_inputs, outputs=encoding_full, name='FullEncoder')
    select_encoder = Model(inputs=feature_inputs, outputs=encoding_select, name='SelectEncoder')
    
    return gsdae_model, full_autoencoder, feature_selector, full_encoder, select_encoder, group_selective_layer

def analyze_feature_importance(group_selective_layer, feature_groups, feature_names):
    """ä¸¤å±‚é‡è¦æ€§åˆ†æ"""
    weights = group_selective_layer.kernel.numpy()
    
    # ç»„é‡è¦æ€§è¯„ä¼°
    group_importance = {}
    for group_name, indices in feature_groups.items():
        if len(indices) > 0:
            group_weights = weights[indices]
            group_l2_norm = np.linalg.norm(group_weights, ord=2)
            group_importance[group_name] = group_l2_norm
    
    sorted_groups = sorted(group_importance.items(), key=lambda x: x[1], reverse=True)
    
    # ç»„å†…å…³é”®ç‰¹å¾è¯†åˆ«
    feature_importance = {}
    for group_name, indices in feature_groups.items():
        if len(indices) > 0:
            group_weights = weights[indices]
            group_features = [feature_names[i] for i in indices]
            feature_weight_pairs = list(zip(group_features, group_weights))
            sorted_features = sorted(feature_weight_pairs, key=lambda x: x[1], reverse=True)
            feature_importance[group_name] = sorted_features
    
    return sorted_groups, feature_importance, weights

def load_and_preprocess_data():
    """æ•°æ®åŠ è½½å’Œé¢„å¤„ç†"""
    # è¯»å–æ•°æ®
    data = pd.read_csv('../data/ä¸¹å‚æ•°æ®salvia_all_20240425 - å‰¯æœ¬.csv')
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}")
    
    # åˆ é™¤å‰ä¸‰åˆ—
    data = data.iloc[:, 3:]
    
    # å®šä¹‰ç›®æ ‡å˜é‡
    target_columns = [
        'CS', 'MT', 'TSIIA', 'TSI', 'DTSI', 'SumTS', 'PD', 'CFA', 'FA', 'SAD', 'SF', 'DSS',
        'SAC', 'SAE', 'MCF', 'RA', 'SAA', 'LA', 'SAY', 'TA', 'CTA', 'MA', 'FMA', 'SUA', 'SAB'
    ]
    
    # åˆ†ç¦»ç›®æ ‡å˜é‡å’Œç‰¹å¾
    available_targets = [col for col in target_columns if col in data.columns]
    target_data = data[available_targets] if available_targets else None
    
    drop_cols = available_targets + ['Soil_sampleN', 'etestN', 'testNp', 'etestpatch']
    feature_data = data.drop(columns=[col for col in drop_cols if col in data.columns])
    
    # æ•°æ®é¢„å¤„ç†
    thresh = len(feature_data) * 0.5
    feature_data = feature_data.dropna(axis=1, thresh=thresh)
    
    if target_data is not None:
        valid_indices = feature_data.dropna().index
        feature_data = feature_data.loc[valid_indices]
        target_data = target_data.loc[valid_indices]
        
        target_valid_indices = target_data.dropna().index
        feature_data = feature_data.loc[target_valid_indices]
        target_data = target_data.loc[target_valid_indices]
    else:
        feature_data = feature_data.dropna()
    
    # ç‹¬çƒ­ç¼–ç 
    categorical_columns = ["Province", "City", "Microb", "Landscape", "SoilType", "soilclass", 
                          "CultivationType", "ClimateType", "æŒ‰æ°”å€™èšç±»åˆ’åˆ†çš„ç±»å‹"]
    categorical_columns = [col for col in categorical_columns if col in feature_data.columns]
    feature_data = pd.get_dummies(feature_data, columns=categorical_columns, drop_first=True)
    
    # å‡†å¤‡ç›®æ ‡å˜é‡
    if target_data is not None and 'SumTS' in target_data.columns:
        main_target = target_data[['SumTS']]
        print("ä½¿ç”¨SumTSä½œä¸ºä¸»è¦ç›®æ ‡å˜é‡")
    elif target_data is not None:
        main_target = target_data.iloc[:, :1]
        print(f"ä½¿ç”¨{target_data.columns[0]}ä½œä¸ºä¸»è¦ç›®æ ‡å˜é‡")
    else:
        main_target = pd.DataFrame(np.random.randn(len(feature_data), 1), columns=['dummy_target'])
        print("åˆ›å»ºè™šæ‹Ÿç›®æ ‡å˜é‡")
    
    print(f"æœ€ç»ˆç‰¹å¾æ•°æ®å½¢çŠ¶: {feature_data.shape}")
    print(f"ç›®æ ‡å˜é‡å½¢çŠ¶: {main_target.shape}")
    
    return feature_data, main_target

if __name__ == "__main__":
    print("ğŸ”„ GSDAE (Group Selective Deep AutoEncoder) ç®€åŒ–ç‰ˆæœ¬")
    print("=" * 60)
    print("ä¸»è¦æ”¹è¿›:")
    print("1. âœ… ç»„ç¨€ç–æ­£åˆ™åŒ– (Group Lasso)")
    print("2. âœ… åŠç›‘ç£å­¦ä¹ æœºåˆ¶") 
    print("3. âœ… é¢„æµ‹å¤´ (Prediction Head)")
    print("4. âœ… å¤åˆæŸå¤±å‡½æ•°")
    print("5. âœ… ä¸¤å±‚é‡è¦æ€§åˆ†æ")
    print("6. âœ… ç‰¹å¾åˆ†ç»„ç»“æ„")
    print("=" * 60)
    print("è¯·è¿è¡Œ danshen_GSDAE_simple.ipynb è¿›è¡Œå®Œæ•´è®­ç»ƒå’Œåˆ†æ")
